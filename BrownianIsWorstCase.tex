\documentclass{article}[12pt]
\usepackage{fullpage}
% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}

\input{macros}

\title{Potential-based hedging algorithms}
\author{Yoav Freund}
\begin{document}

\maketitle
\begin{abstract}
  We study regret-minimizing online algorithms based on potential
functions. First, we show that any algorithm with a regret bound that
holds for any $\epsilon$ is equivalent to a potential minimizing
algorithm and vice versa. Second we should a min-max learning
algorithm for known horizon. We show a regret bound that is close to
optimal when the horizon is not known. Finally we give an algorithm
with second order bounds that characterize easy sequences.
\end{abstract}

\section{Introduction}
Online prediction with expert advise has been studied extensively over
the years and the number of publications in the area is vast (see
e.g.~\cite{vovk1990aggregating, feder1992universal,
  littlestone1994weighted, cesa1997use, cesa2006prediction}.

Here we focus on a simple variant of online prediction with expert
advice called {\em the decision-theoretic online learning game}
(DTOL)~\cite{freund1997decision}, we  consider the 
  signed version of this game.

DTOL (Figure~\ref{fig:DTOL}) is a repeated zero sum game between a {\em learner} and an {\em
  adversary}. The adversary controls the losses of $N$ actions, while
the learner controls a distribution over the actions.

\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
For $i=1,\ldots,T$
\begin{enumerate}
    \item The learner chooses a weight function $w_j^i$ over the
      actions $j \in \{1,\ldots,N\}$. 
    \item The adversary chooses an {\em instantaneous loss} for each
      of the $N$ actions: \\
      $l_j^i \in [-1,+1]$ for $j \in \{1,\ldots,N\}$.
    \item The {\em cumulative loss of action $j$} 
    is  $L^i_j = \sum_{s=1}^i l_j^s$. 
    \item The learner incurs an {\em instantanous average loss} defined as
      $\ell^i = \frac{\sum_{j=1}^N w_j^i l_j^i}{\sum_{j=1}^N w_j^i}$
    \item The {\em cumulative loss of the learner} is
      $L_\ell^i = \sum_{s=1}^i \ell^s$
    \item The {\em cumulative regret} of the learner with respect to
      action $j$ is
      $\R_j^i = L_\ell^i -L_j^i $.
\end{enumerate}
\end{minipage}}
\caption{Decision theoretic online learning \label{fig:DTOL}}
\end{figure}

The goal of the learner (in the percentile version of the game) is to
perform almost as well as $k$ best actions. Specifically, we sort the
regrets in decreasing order
$\R_1^i \geq \R_2^i \geq \cdots \geq \R_k^i \geq \cdots$ and define
$\R_k^i$ to as the regret relative to the $\epsilon=k/M$ top
percentile, denote $\R_\epsilon^i$. Our goal is to find algorithms
that guarantee small upper bounds on $\R_\epsilon^T$. Known bounds
have the form $c \sqrt{T \ln {1/\epsilon}}$, but the algorithm has to
be tuned based on prior knowledge of $\epsilon$. We seek algorithms
with regret bounds that hold {\em simultanously} for all values of
$\epsilon$. In other words algorithms that do not need to know
$\epsilon$ or $t$ ahead of time. The following definition formalizes
the concept of simultanous bounds:
\begin{definition}[Simultanous regret
  bound] \label{def:unif-regret-bound} Let $G: \reals \to [0,1]$ be a
  non-increasing function which maps regret bounds to probabilities.
  A distribution over regrets $\state$ is simultanously bound by $G$ if
  \[
    \forall r \in \reals \;\; \P{\rho \sim \state(T)}{\rho \geq r} \leq G(r)
  \]
\end{definition}


A potential function is an increasing function
$\pot:\reals \to \reals$. Potential based learing algorithm are
designed to bound the the average potential
$\E{\R \sim \state}{\pot(\R)}$. Potential functions have long been
used to design and analyze online learning algorithms. The novelty 
here is that we consider minimizing the average potential as a goal in itself.
\begin{definition}[Average potential bound] \label{def:aver-potential-bound}
  A distribution over he reals $\state$ satisfies the average
  potential function $\pot$ if
  $$\E{\R \sim \state}{\pot(\R)} \leq 1$$
  Where $\pot: \reals \to \reals^+$ is a non decreasing function. 
\end{definition}

The next theorem identifies a one to one relationship between
simultanous bounds and average potential bounds. 
\begin{theorem}\label{thm:simulBoundAveragePot}
 A distribution $\state$ is simultanously bounded by $B$ if and only
 if it satisfies the average potential bound with $\phi(\R) = B(\R)^{-1}$
\end{theorem}
The proof of the theorem is in Appendix~\ref{proof:simulBoundAveragePot}.

Simultanous regret bounds are more intuitive than potential
functions. On the other hand potent, but average potential
bounds lend themselves to analysis and optimization. In 



given the potential function at the end of the game $\pot(T,\R)$ and
the strategies used by the learner and the adversary, we can define a
potential $\pot(T-1,\R)$ such that the average potentials are equal:
\[
  \E{\R \sim \state(T-1)}{\pot(T-1,\R)} = \E{\R \sim \state(T)}{\pot(T,\R)}
\]
In Section~\ref{sec:potentials} we that this equation can be
used to create a potential function of all iterations, and how it can
be used to find the optimal strategies for both learner and adversary.

  Our ultimate result are min-max optimal bounds. However, there seem
  to be no matching min-max strategies for the original DTOL. To
  achieve min-max optimality we extend the game by enlarging the set of
  choices available to the adversary.  As the learner does not get
  additional choices, the min/max bound for the extended game is an
  upper bound on the average potential in the original game.

The rest of the paper is organizes as follows.

TBD

\section{related work}
Most of the papers on potential based online algorithms consider
one or a few potential functions. Most common is the exponential
potential, but others have been considered~\cite{cesa2006prediction}.
A natural question is what is the difference between potential
functions and whether some potential function is ``best''.

In this paper we consider a large set of potential functions,
specifically, potential functions that are strictly positive and have
strictly positive derivatives of orders up to four. The exponential
potential and the NormalHedge potential~\cite{chaudhuri2009parameter,luo2015achieving}
are member of this set. 

To analyze these potential functions we define a different
game, which we call the ``potential game''. In this game the primary
goal of the learner is not to minimize regret, rather, it is to
minimize the final score $\score^T$. To do so
we define potential functions for intermediate steps: $0 \leq t
<T$.\footnote{The analysis described here builds on a long line of
  work. Including the Binomial Weights algorithm and it's
  variants~\cite{cesa1996line,abernethy2006continuous,abernethy2008optimal}
  as well as drifting games~\cite{schapire2001drifting,freund2002drifting}.}

Zero-order bounds on the regret ~\cite{freund1999adaptive} depend only on $N$
and $T$ and typically have the form
\begin{equation} \label{eqn:0-order-bound}
  \max_j \R_j^T < C E \sqrt{T \ln N}
\end{equation}
for some small constant $C$ (typically smaller than 2).
These bounds can be extended to infinite sets of experts by defining
the $\epsilon$-regret of the algorithm as the regret with respect to
the best (smallest-loss) $\epsilon$-percentile of the set of experts.

this replaces the bound~(\ref{eqn:0-order-bound}) with 
\begin{equation} \label{eqn:0-epsilon-order-bound}
  \max_j \R_j^T < C E \sqrt{T \ln \frac{1}{\epsilon}}
\end{equation}

Lower bounds have been proven that match these upper bounds up to a
constant. These lower bounds typically rely on constructions in which
the losses $l_j^i$ are chosen independently at random to be either
$+1$ or $-1$ with equal probabilities.



Several algorithms with refined upper bounds on the regret have been
studied. Of those, the most relevant to our work is a paper by 
Cesa-Bianchi, Mansour and
Stoltz~\cite{cesa2007improved} on second-order regret bounds.
The bound given in Theorem~5 of ~\cite{cesa2007improved} can be
written, in our notation, as:
\begin{equation} \label{eqn:2nd-order-bound}
  \max_j \R_j^T \leq 4\sqrt{V_T \ln N} +2 \ln N +1/2 
\end{equation}
Where
\[
  \var_i = \sum_{j=1}^N P^i_j (l_j^i)^2 -  \left( \sum_{j=1}^N P^i_j
    l_j^i \right)^2 \mbox{ and } \V_T= \sum_{i=1}^T \var_i
\]

A few things are worth noting. First, as $|l_j^i|\leq 1$,
$\var_j\leq 1$ and therefor $V_T\leq T$. However $\V_T/T$ can be
arbitrarily small, in which case inequality~\ref{eqn:2nd-order-bound}
provides a tighter bound than ~\ref{eqn:0-order-bound}. Intuitively,
we can say that $\V_T$ replaces $T$ in the regret bound. This paper
provides additional support for replacing $T$ with $\V_T$ and provides
lower and upper bounds on the regret involving $\V_T$.

\section{Main Results}
\begin{enumerate}
\item {\bf Unifrm regret bound} There exists an online learning
  algorithm such that for any $\nu>0$ (set in advance) and any
  $t,\epsilon$ (holds uniformly) the following regret bound holds.
  \begin{equation}
\R_\epsilon \leq \sqrt{(t+\nu) \left( \ln (t+\nu) + 2 \ln \frac{1}{\epsilon}\right)}
\end{equation} 
\item {\bf Second order bound}
\item {\bf optimality of Brownian motion} For any potential function
  in $\SP{4}$ the min/max value of any state $(t,R)$ is attained by
  Brownian motion on the part of the adversary for any $s\geq t$.
\end{enumerate}


\section{Preliminaries} \label{sec:preliminaries}

We define some notation that will be used in the rest of the paper.

{\bf Positivity}
We require that potential functions have positive derivatives for a
range of degree. To that end we use the following definition:
\begin{definition}[Strict Positivity of degree $k$]
A function $f:\reals \to \reals$ is strictly positive of degree $k$, 
denoted $f \in \SP{k}$ if the derivatives of orders 0 to $k$:  
$f(x), \frac{d}{dx}f(x), \ldots, \frac{d^k}{dx^k}f(x)$ exist and are strictly positive.
\end{definition}
For the integer time game we assume that $\pot(\cdot) \in
\SP{2}$. Later on, in section~\ref{sec:smallsteps}, we will further
restrict our potential functions to be in $\SP{4}$.

{\bf Divisibility:} To reach optimality we need the set of actions to
be arbitrarily divisible. Intuitively, We replace the finite set of
actions with a continuous mass, so that each set of actions can be
partitioned into two parts of equal weight.  Formally, we define the
set of actions to be a probability space $(\Omega,\sigma,\mu)$ such
that $\omega \in \Omega$ is a particular action. We require that the
space is {\em arbitrarily divisible}, which means that for any
$s \in \sigma$ , there exist a partition $u,v \in \sigma$ such that
$u \cup v = s, u \cap v = \emptyset$, and
$\P{}{u}=\P{}{v}=\frac{1}{2} \P{}{s}$.

{\bf State:} The {\em state} of a game at iteration $i$, denoted $\state(i)$, is
a random variable that maps each action $\omega \in \Omega$ to the
cumulative regret of $\omega$ at time $i$: $\R_\omega^i$. The sequence
of cumulative regrets corresponding to action $\omega$ is the {\em
  path} of $\omega$:
\begin{equation} \label{eqn:path}
  S_{\omega}=(\R_\omega^1,\R_\omega^2,\ldots,\R_\omega^N)
\end{equation}

{\bf Expected value shorthand:} Suppose $P$ is a distribution over the reals, and $f:\reals
\to \reals$, we use the following short-hand notation for the expected
value of $f$ under the distribution $P$:
\[ P \odot f \doteq \E{x \sim P}{f(x)}  \]
We define the {\em score} at iteration $i$ as the average potential
with respect to the state:
\[ \score(i) = \state(i) \odot \pot(i) \doteq \E{\R \sim \state(i)}{\pot(i,\R)}\]
Note that in this short-hand notation we suppress the variable with
respect to which the integration is defined, which will always be $\R$.

{\bf Convolution:} Let $A,B$ be two independent random variables. We define the
convolution $A \oplus B$ to be the distribution of $x+y$. A constant
$a$ corresponds to the point mass distribution concentrated at
$a$. For convenience we define $A \ominus B = A \oplus (-B)$


\section{Integer time game}

The integer time game is described in
Figure~\ref{fig:integerTimeGame}.  The integer time game generalizes
the decision theoretic online learning problem~\cite{FreundSc97} in
the following ways:
\begin{enumerate}
\item The goal of the learner in DTOL is to guarantee an upper bounds
  on the regret. The learner's goal in the integer time game is to
  minimize the final score. From theorem~\ref{thm:simulBoundAveragePot} we know that if
  we set the potential as $\pot(R) = \frac{1}{G(x)}$ then the two
  conditions are equivalent, allowing us to focus on the score.
\item The number of iterations $T$ is given as input, as is the
  potential function at the end: $\pot(T,\R)$.
\item The actions are assumed to be {\em divisible}. For our purposes
it is enough to assume that any action can be split into two equal
weight parts.
\end{enumerate}



The key to the potential based analysis is that usiing the predefined
final potential we can define potential functions and scores for all
iterations $1,\ldots,T-1$. This is explained in the next subsection.

\begin{figure}[ht]
\framebox{
\begin{minipage}[t]{6.4in}
Initialization:
\begin{itemize}
\item input: $T$ : The number of iterations.
\item Final iteration potential function:  $\phi(T,\R) \in \SP{2}$
\item $\state(1) = \delta(0)$ is the initial state of the game which
  is a point mass distribution at 0. 
\end{itemize}

For $i=1,2,\ldots,T$:\\

\begin{enumerate}
\item The learner chooses a non-negative random variable over $\Omega$
  that is the {\em weight function} $\learnerM(i,\R)$ such that
  $\state(i) \odot \learnerM(i)=1$
\item The adversary chooses a function $\adversM(i,\R)$ that maps
  $i,\R$ to a distribution over $[-1,+1]$. This
  random variable corresponds to the instantanuous loss of each action at
  time $t$.
\item 
  We define the {\em bias} at $(i,\R)$ to be
  \begin{equation} \label{eqn:Bias}
    \Bias(i,\R) \doteq \E{l \sim \adversM(i,\R)}{l}
  \end{equation}
\item the average loss is 
  \begin{equation} \label{eqn:aggregate-loss}
    \ell(i)=\state(i,\R) \odot \paren{\learnerM(i,\R) \Bias(i,\R)}
  \end{equation}
\item The state is updated. 
  \begin{equation} \label{eqn:state-update}
    \state(i+1) = \E{\R \sim \state(i)}{\R \oplus  \adversM(i,\R)}
    \oplus -\ell(i)
    \end{equation}
  Where $\adversM(i,\R)$ is the distribution of the losses of experts
  with respect to which the regret is $\R$ after iteration
  $i-1$. $\oplus$ denotes the convolution as defined above.
\end{enumerate}

The final score is calculated: $\score(T)=\state(T) \odot
\pot(T)$.

The goal of the learner is to minimize this score, the goal
of the adversary is to maximize it.
\end{minipage}}
\caption{The integer time game \label{fig:integerTimeGame}}
\end{figure}

\subsection{Defining potential Functions for all iterations\label{sec:potentials}}

The potential game defines the {\em final} potential function
$\pot(T)$, at the end of the game. We will now show, that we can
extend the definition of a potential function to all iterations of the game.

 A single action defines a path $S_\omega$ (as defined in (\ref{eqn:path})). Fixing
 the strategies of the learner and the adversary determines
 a distribution $\D$ over paths.
We describe two equivalent ways to define $\potPQ(i,\R)$ for $i<T$
 \begin{enumerate}
 \item{\bf Using conditional expectation} We can define the potential
   on iteration $i$ based on the fixed potential at iteration $T$.
Using the definition of prefix sum given
in Equation~(\ref{eqn:prefix-sum}, we can write this conditional
expectation as 
\begin{equation}
  \forall i=1,\ldots,T, \R \;\;\;
  \potPQ(i,\R)=\E{\omega\sim \D|\R_\omega^i=\R}{\pot(T,\R_{\omega}^T)}
\end{equation}
 \item{\bf Using backward induction} It is sometimes convenient to
   compute the the potential for time $i$ from the potential at time
   $i+1$:
   \begin{equation} \label{eqn:back-induction}
     \forall i=1,\ldots,T-1, \R\;\;\;
     \potPQ(i,\R)=\E{\omega\sim \D|M_\omega^i=\R}{\potPQ(i+1,\R_{\omega}^{i+1})}
   \end{equation}
by using backwards induction: $i=T-1,T-2,\ldots,1$ we can compute the
potential for all iterations.


We use Equations~(\ref{eqn:Bias},\ref{eqn:aggregate-loss}) and
marginalizing over $\R$ to express Equation~(\ref{eqn:back-induction})
in terms of the single step strategies:
\begin{equation} \label{eqn:induction}
  \forall i=1,\ldots,T-1, \R\;\;\;
  \potPQ(i,\R) \doteq \E{r \sim [(\R-\ell(i)) \oplus \adversM(i,\R)]}{\potPQ(i+1,r)}
\end{equation}

\end{enumerate}

The score at iteration $i$ is defined as
$\score(i)=\state(i)\odot \pot(i)$. The scores are all different
expressions for calculating the expected final potential for the fixed strategies
$\adversM, \learnerM$. Therefor the scores are all equal, as expressed
in the following theorem:

\begin{theorem} \label{thm:backward-recursion}
Assuming $ \learnerM(i,\R), \adversM(i,\R)$ are fixed for all
$i=1,\ldots,T-1$, then
\[
  \state(T)\odot \pot(T)=\score(T)=\score(T-1)=\cdots=\score(1)=\potPQ(1,0)
  \]
\end{theorem}

A few things worth noting:
\begin{enumerate}
\item $\potPQ(i,\R)$ is the the final expected potential
  given that the paths starts at $(i,\R)$ and that
  the strategies used by both players in iterations $i,\ldots,T$ are fixed. Note
  also that which strategies were used in iterations $1,\ldots,i-1$ is
  of no consequence. The effect of past choices is captured by the
  state $\state(i)$.
\item
  The final expected potential is equal to $\pot(1,0)$ which is the
  potential at the common starting point: $i=1$, $\R=0$.
\end{enumerate}

\subsection{Upper and Lower potentials}
Next,we vary the strategies of one side or the other to define upper
and lower potentials.
\begin{equation} \label{eqn:upperPotentials}
  \exists \learnerM, \;\;\; \forall \adversM,\;\; \forall 1\leq i \leq
  T,\;\; \forall \R \in \reals,\;\; \upperpot(i,\R) \geq \potPQ(i,R)
\end{equation}
\begin{equation} \label{eqn:lowerPotentials}
  \exists \adversM, \;\;\; \forall \learnerM, \;\; \;\; \forall 1\leq i \leq
  T,\;\; \forall \R \in \reals,\;\; \lowerpotb(i,\R) \leq \potPQ(i,R)
\end{equation}

In words, $\upperpot$ is an upper bound on the potential that is 
guaranteed by the learner strategy $\learnerM$ while $\lowerpotb$
is a lower bound that is guaranteed by the adversarial
strategy $\adversM$.

Our ultimate goal is to find strategies $\learnerM$ and
$\adversM$ such that
\begin{equation} \label{eqn:limitPotential}
\forall i,\R,\;\;\; \lowerpot(i,\R) = \upperpotj(i,\R)
\end{equation}
However, this will have expand the game in several ways.
We start by analyzing the integer time game. 


\subsection{Strategies for the integer time  game} \label{sec:strat-integer}

We assume that $\pot(T)  \in \SP{2}$, in other words, the final
potential is positive, increasing and convex.

We define a particular adversarial strategy
\begin{equation} \label{eqn:adv-strat-p}
  \adversM^*(i,\R) =
  \begin{cases}
    +1 & \mbox{ w.p. } \frac{1}{2}\\
    -1 & \mbox{ w.p. } \frac{1}{2}\\
  \end{cases}
\end{equation}

and a particular learner strategy
\begin{equation} \label{eqn:learner-strat-1}
\learnerM^*(i,\R) = \frac{1}{Z} \frac{\pot(i,\R+2) - \pot(i,\R-2)}{2}
\end{equation}
Where $Z$ is a normalization factor
$$Z = \E{\R \sim \state(i)}{\frac{\pot(i,\R+2) - \pot(i,\R-2)}{2}}$$

We next give upper and lower bounds on the final average potential
based on these strategies.

Let $B(n,a)$ denote the distribution over the reals defined by
$\sum_{i=1}^n X_i$ where $X_i$ are iid binary random variables which
attain the values $-a,+a$ with equal probabilities.

\begin{theorem} \label{thm:IntegerGameBounds}
  Let $\pot_T \in \SP{2}$, for any iteration $0 \leq i \leq T$ and
  initial regret $\R_0 \in \reals$ we define $\state(i,\R_0)$ to
  contain all paths that are equal to $\R_0$ on iteration $i$. We
  consider the final score $\score(T)$ starting from state
  $\state(i,\R_0)$ and using a particular strategy
  \begin{itemize}
  \item
    The adversarial strategy~(\ref{eqn:adv-strat-p}) starting from
    $\state(i,\R_0)$. Guarantees a final potential 
     $$ \score(T) \geq \E{\R \sim \R_0 \oplus B(T-i,1)}{\pot(T,\R)} $$
  \item
    There learner strategy~(\ref{eqn:learner-strat-1}) guarantees 
    $$\score(T) \leq \E{\R \sim \R_0 \oplus B(T-i,2)}{\pot(T,\R)}$$
  \end{itemize}
\end{theorem}


The next Lemma is the main part of the proof ot
Theorem~(\ref{thm:IntegerGameBounds}). We use the backward induction
from Theorem~(\ref{thm:backward-recursion}) To compute upper and lower
potentials (Equations~(\ref{eqn:upperPotentials},\ref{eqn:lowerPotentials})) for
Strategies~(\ref{eqn:adv-strat-p}) and~(\ref{eqn:learner-strat-1})

The last iteration of the game: $i=T$ is the first step of the
backward induction. The uper and lower bounds are both set equal to
the first step in the backward induction we define
$$  \lowerpotb(T,\R) = \upperpotb(T,\R) = \pot(T,\R) $$

\begin{lemma} \label{lemma:first-order-bound}
  If $\pot(i,\R) \in \SP{2}$
  \begin{enumerate}
    \item The adversarial strategy
    guarantees the lower potential
 \begin{equation} \label{eqn:backward-iteration-lower}
   \lowerpotb(i, \R) = \frac{\lowerpotb(i,\R+1) + \lowerpotb(i,\R-1)}{2}
 \end{equation}
   
    \item The learner strategy:
      guarantees the upper potential 
      \begin{equation} \label{eqn:backward-iteration-upper-recursion}
        \upperpotb(i, \R) = \frac{\upperpotb(i,\R+2) + \upperpotb(i,\R-2)}{2}
      \end{equation}
    \end{enumerate}
    Assuming the adversary uses strategy \ref{} in
    each iteration we get that 
\end{lemma}

\proof
\begin{enumerate}
\item By symmetry adversarial strategy~(\ref{eqn:adv-strat-p}) guarantees that
  the aggregate loss~(\ref{eqn:aggregate-loss}) is zero regardless of
  the choice of the learner: $\ell(i)=0$.
  Therefor the state update~(\ref{eqn:state-update}) is equivalent to
  the symmetric random walk:
  $$\state(i) = \frac{1}{2} \paren{(\state(i) \oplus 1) + (\state(i)
    \ominus 1)}$$
  Which in turn implies that if the adversary plays $\adversM^*$
  and the learner plays an arbitrary strategy $\learnerM$
  \begin{equation} \label{eqn:lower}
    \lowerpotb(i-1,\R) = \frac{\lowerpotb(i,\R-1)+\lowerpotb(i,\R+1)}{2}
  \end{equation}
  As this adversarial strategy is oblivious to the learner's strategy, it
  guarantees that the average value at iteration $i$ is {\em equal} to the
  average of the lower value at iteration $i$.
\item
  Plugging learner's strategy~(\ref{eqn:learner-strat-1})
  into equation~(\ref{eqn:aggregate-loss}) we find that
 \begin{equation} \label{eqn:ell-optimal-learner}
   \ell(i) = \frac{1}{Z_{i}} \E{\R \sim \state(i)}{\paren{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}
   \Bias(i,\R)}
\end{equation}
  Consider the score at iteration $i$ when the learner's strategy
  is $\learnerM^*$ and the adversarial strategy is arbitrary $\adversM$:
  
   \begin{equation} \label{eqn:Pot-Update}
    \score_{\learnerM^*,\adversM}(i,\R) = \E{\R \sim \state(i)}{ \E{y \sim
      \adversM(i)(\R)}{\pot(i,\R+y-\ell(i))}}
  \end{equation}
  As $\pot(i,\cdot)$ is convex and as $(y-\ell(i)) \in [-2,2]$,
  \begin{equation} \label{eqn:pot-upper}
    \upperpotb(i-1,\R+y) \leq \frac{\upperpotb(i,\R+2)+\upperpotb(i,\R-2)}{2} +
    (y-\ell(i)) \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}
    \end{equation}
  Combining the equations~(\ref{eqn:ell-optimal-learner}) and~(\ref{eqn:Pot-Update}) we find that
  \begin{eqnarray}
  \score_{\learnerM^*,\adversM}(i,\R)&=&\E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{\upperpotb(i,\R+y-\ell(i))}}\\
  &\leq & \E{\R \sim \state(i)}{\frac{\upperpotb(i,\R+2)+\upperpotb(i,\R-2)}{2}}\\
  &+&
  \E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{(y-\ell(i)) \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}} \label{eqn:zero-term}
  \end{eqnarray}
  
The final step is to show that the term~(\ref{eqn:zero-term}) is equal
to zero. As $\ell(i)$ is a constant with respect to $\R$ and $y$ the
term~(\ref{eqn:zero-term}) can be written as:
\begin{eqnarray}
&&\E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{(y-\ell(i))
   \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}}\\
&=&
\E{\R \sim \state(i)}{\Bias(i,\R)
    \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}\\
  &-& \ell(i) \E{\R \sim \state(i)}{
    \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}\\
  &=& 0
\end{eqnarray}
\end{enumerate}
\qed

Theorem~\ref{thm:IntegerGameBounds} follows directly from Lemma~\ref{lemma:first-order-bound}


\section{From integer to discrete time}
\label{sec:discrete}

The upper and lower bound on the final score given in
Theorem~\ref{thm:IntegerGameBounds} do not match. There is a strict
inequality between the upper and lower
bonds:$\E{\R \sim \R_0 \oplus B(T-i,1)}{\pot(T,\R)} < \E{\R \sim \R_0
  \oplus B(T-i,2)}{\pot(T,\R)}$. In other words, the
strategies~(\ref{eqn:adv-strat-p},\ref{eqn:learner-strat-1}) are not a
min/max pair.\footnote{There might be other (pure) strategies for the integer
game that are a min/max pair, we conjecture that is not the case, and
seek a extension of the game that would yield min/max strategies.}

To close the gap we extend the game, preserving the integer step game
as a special case. Our extension increases the set of possibilities
available to the adversary but not to those available to the
learner. As a result, any learner strategy that bound on the final
score in the extended game holds also for the integer game. We call
the extended game the {\em discrete time game}.
Figure~(\ref{fig:discrete-Time-Game}) describes the Discrete-time
game.

%And design it around the advesarial strategy
%$\adversM^I(i,\R)$ (Equation~\ref{eqn:adv-strat-p}).

In the integer time game the loss of each action is in the range
$[-1,+1]$, in the discrete time game the adversary chooses, on
iteration $i$ a step size $0<s_i\leq 1$ which restricts the losses to
be in the range $[-s_i,+s_i]$. Note that by always choosing $s_i=1$,
the adversary can recreate the integer time game.

\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
Initialization: $t_0=0$ \newline
  
On iteration $i=1,2,\ldots$
\begin{enumerate}
\item  If $t_{i}= \realT$ the game terminates.
\item The adversay chooses a {\em step size} $0<s_i\leq \min(\sqrt{1-t_i})1$, which advances
  time by $t_i = t_{i-1}+s_i^2$
\item Given $s_i$, the learner chooses a distribution $\learnerM(i)$ over $\reals$.
\item The adversary chooses a mapping from $\reals$ to distributions
  over $[-s_i,+s_i]:\;\adversM(t): \reals \to \Delta^{[-s_i,+s_i]}$
  
\item The aggregate loss is calculated:
  \begin{equation} \label{eqn:ell-discrete}
    \ell(t_i)=\E{\R \sim \state(t_i)}{\learnerM(t_i,\R) \Bias(t_i,\R)}
    \mbox{ where } \Bias(t_i,\R) \doteq \E{y \sim \adversM(t_i,\R)}{y}
  \end{equation}
  Such that $|\ell(t_i)| \leq s_i^2$
\item The state is updated. The expectation below is over
  distributions. and the notation $G \oplus \R$ means
  that distribution $G$ over the reals is shifted by the amount
  defined by the scalar $\R$:
  $$\state(t_i) = \E{\R \sim \state(t_{i})}{\adversM(t_i)(\R)\oplus (\R-\ell(t_i))}
  $$
\end{enumerate}

Upon termination, the final value is calculated:
$$\score(\realT) =\E{\R \sim \state(\realT)}{\pot(\realT,\R)}$$

\end{minipage}}
\caption{The discrete time game  \label{fig:discrete-Time-Game}}
\end{figure}

We make two additional changes to the integer time game in order to define
the discrete time game, we first list them and then provide justifications.
\begin{enumerate}
\item {\bf real-valued time} In the integer time game we use an
  integer to indicate the iteration number: $i=1,2,\ldots,T$. In the
  discrete time game we use an positive real value, which we call
  ``time'' and use the update rule $t_{i+1} = t_i + s_i^2$, and define
  the final time, which is used in the regret bound, to be
  $\realT=\sum_{i=0}^T s_i^2$
\item {\bf Bounded average loss} We restrict the average loss to a
  range much smaller than $[-s_i,+s_i]$, specifically:
  $|\ell(i)| \leq s_i^2$
\end{enumerate}
Note that both of these conditions hold trivially when $s_i=1$

To justify these choices we consider a parametrized version of
the adversarial strategy in the integer time game (\ref{eqn:adv-strat-p})
\begin{equation} \label{eqn:adv-strat-s}
  \adversM^d(\cdot,\cdot,s,p) =
  \begin{cases}
    +s & \mbox{ w.p. } p\\
    -s & \mbox{ w.p. } 1-p\\
  \end{cases}
\end{equation}

We denote by $\lowerpot(T,s,p)$ the lower potential that is guaranteed
by the adversarial strategy $\adversM^d(\cdot,\cdot,s,p)$ in a game with
$T$ iterations. Recall that, for $\adversM^d$, the state evolution
does not depend on the learner's strategy. It is not hard to
show that the final state is $B(T,1)$ when $s=1$ and $B(T,1/2)$ when $s=1/2$.

Suppose $T$ is large enough that the normal approximation for the
binomial can be used. Let ${\cal N}(\mu,\sigma^2)$ be the normal
distribution with mean $\mu$ and variance $\sigma^2$. Then the normal
approximations are:
$B(T,1) \approx {\cal N}(0,T)$ and $B(T,1/2) \approx {\cal N}(0,T/4)$,
In other words,
$$\lowerpot(T,s,1/2) \approx {\cal N}(0,Ts^2) \odot \pot(T)$$

In the general case the step size $s_i$ is  different in different
iterations, and the total variance that we get from using
$\adversM^s(\cdot,\cdot,s_i,1/2)$ for $i=1,\ldots,T$ is
$V_T=\sum_{i=1}^{T-1}s_i^2$. The cumulative variance $V_T$ is a better measure of the
length of the game than the number of iterations $T$, because it takes
into account the range $s_i$ of the predictionsl. This motivates
us to define a real valued time $t_i$ where $t_0=0$ and $t_i =
t_{i}+s_i^2$ and $t_T = {\cal T}$ \
This completes our justification of the real valued time.

Next, we explain why we bound the average loss. Consider repeating the
adversarial strategy \linebreak
$\adversM^d(\cdot,\cdot,s,\frac{1}{2}+\epsilon)$ until the cumulative
variance is $\realT$.  This means tht $s^2 T(1/4-\epsilon^2)=\realT$,
or that $T = \frac{c}{\epsilon^2}$ for some constant $c$. As the
cumulative mean is $2sT \epsilon$, we get that
$2s \frac{c}{s^2} = \frac{2c}{s}$. This implies that, as $s \to 0$ the
cumulative mean from $1$ to $T$ increases to $\infty$, which makes the
regret non-sensical. We correct that by requiring the
$\epsilon = c s$, or, in other words, that the magnitude of the
expected loss $\ell(i)$ is bounded by $s_i^2$.

\subsection{Strategies for discrete time game}
Now that the game is defined, we define a sequence of strategies for
the adversary and a single strategy for the learner. These strategies
are scaled versions of the strategies for the integer time
game~(\ref{eqn:adv-strat-p},\ref{eqn:learner-strat-1}).

For adversarial strategy $S_k$ we set the step size to
$s_k=2^{-k}$ and use the adversarial strategy
$\adversM^d(\cdot,\cdot,s_k,1/2)$.

Following the same line of argument as the first part of the proof of
Lemma~\ref{lemma:first-order-bound} we consider the time points:
$t_i=i s_k^2=i 2^{-2k}\realT$ for $i=0,1,\ldots,2^{2k}$.

  \begin{equation} \label{eqn:lower-discrete}
    \lowerpotd(t_{i-1},\R) = \frac{\lowerpotd(t_i,\R-s_k)+\lowerpotd(t_i,\R+s_k)}{2}
  \end{equation}

The learner's strategy depends on $s_k$:
(Eqn~\ref{eqn:learner-strat-1}). We follow the same line of argument as the second part of the proof of
Lemma~\ref{lemma:first-order-bound} to give a recursion for the upper
potential. The citical difference between the integer game is and the
dicrete game is that in the discrete game $\ell(t_i)\leq s_i^2$ which
implies that $(y-\ell(t_i)) \in [-s_k(1+s_k),s_k(1+s_k)]$. This yields 
\begin{eqnarray} \label{eqn:learner-strat-1c}
  \learnerM^{d}(t_{i},\R) = \frac{1}{Z^{1d}}
  \frac{\pot(t_i,\R+s_k(1+s_k)) -
  \pot(t_i,\R-s_k(1+s_k))}{2} \\
  \mbox{ where } Z^{1d} = \E{\R \sim \state(t_i)}{\frac{\pot(t_i,\R+s_k(1+s_k)) -
  \pot(t_i,\R-s_k(1+s_k)}{2}} \nonumber
\end{eqnarray}

Relate back to the corresponding line in the integer game.

  \begin{eqnarray}
  \score_{\learnerM^d,\adversM}(t_i,\R)&=&\E{\R \sim \state(i)}{\E{y \sim \adversM(t_i,\R)}{\upperpotd(t_i,\R+y-\ell(t_i))}}\\
  &\leq & \E{\R \sim \state(t_i)}{\frac{\upperpotd(i,\R+s_k(1+s_k))+\upperpotd(i,\R-s_k(1+s_k))}{2}}\\
  &+&
      \E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{(y-\ell(i))
      \frac{\upperpotd(i,\R+1+s_i^2)+\upperpotd(i,\R-1-s_i^2)}{2}}}
%      \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}} \label{eqn:zero-term}
  \end{eqnarray}






We start with the high-level idea. Consider iteration $i$ of the
continuous time game. We know that the adversary prefers $s_i$ to be
as small as possible. On the other hand, the adversary has to choose
some $s_i>0$. This means that the adversary always plays
sub-optimally. Based on $s_i$ the learner makes a choice and the
adversary makes a choice. As a result the current state $\state(t_{i})$
is transformed to $\state(t_i)$. To choose it's strategy, the learner
needs to assign value possible states $\state(t_i)$. How can she do
that? By assuming that in the future the adversary will play
optimally, i.e. setting $s_i$ arbitrarily small. While the adversary
cannot be optimal, it can get arbitrarily close to optimal, which is
brownian motion.

Note that the learner chooses a distribution {\em after} the adversary
set the value of $s_i$. The discrete time version of $\learnerM^1$

In the discrete time game the adversary has an additional choice, the
choice of $s_i$. Thus the adversary's strategy includes that choice.
There are two constraints on this choice: $s_i \geq 0$ and
$\sum_{i=1}^n s_i^2 = T$. Note that even that by setting $s_i$
arbitrarily small, the adversary can make the number of steps - $n$ -
arbitrarily large. We will therefor not identify a single adversarial
strategy but instead consider the supremum over an infinite sequence
of strategies.

\begin{theorem} \label{thm:seq-of-adv-strategies}
  ~\\
  Assume $\pot(\realT,\R) \in \SP{4}$ and 
   let $A = N(0,\sqrt{\realT}) \odot \pot(\realT)$. then
   for any $\epsilon>0$
   \begin{itemize}
     \item
    There exists a strategy for the adversary that guarantees, against
    any learner, that $\score(T) \geq A-\epsilon$
  \item
    There exists a strategy for the learner that guarantees, against
    any adversary, that $\score(T) \leq A+\epsilon$.
  \end{itemize}
\end{theorem}


\subsection{The adversary prefers smaller steps} \label{sec:smallsteps}
As noted before, if the adversary chooses $s_i=1$ for all $i$ the game
reduces the the integer time game. The question is whether the
adversary would prefer to stick with $s_i=1$ or instead prefer to use
$s_i<1$. In this section we give a somewhat surprising answer to this question
-- the adversary {\em always} prefers a smaller value of $s_i$ to a larger
one. This leads to a preference for $s_i \to 0$, as it turns out, this
limit is well defined and corrsponds to Brownian motion, also known as
Wiener process.

Consider a sequence of adversarial strategies $S_k$ indexed by
$k=0,1,2,$. The adversarial strategy $S_k$ is corresponds to always
choosing $s_i = 2^{-k}$, and repeating  $\adversM^{1/2}_{\pm 2^{-k}}$ 
for $T 2^{2k}$ iterations.
This corresponds to the distribution created by a random walk with
$T 2^{2k}$ time steps, each step equal to $+2^{-k}$ or  $-2^{-k}$ with probabilities $1/2,1/2$.
Note that in order to preserve the variance, halving the step size
requires incresing the number of iterations by a factor of four.

Let $\pot(S_k,t,\R)$ be the value associated with adversarial
strategy $S_k$, time $t$ (divisible by $2^{-2k}$) and
location $\R$. We are ready to state our main theorem.

\begin{theorem}\label{thm:smallerSteps}
  If the final value function has a strictly positive fourth
  derivative:
  $$ \frac{d^4}{d \R^4} \finalPot(\R) >0, \forall \R$$
  then for any integer $k>0$ and any $0 \leq  t \leq T$, such that $t$
  is divisible by
  $2^{-2k}$ and any $\R$,
  $$\pot(S_{k+1},t,\R)) >  \pot(S_{k},t,\R)$$
\end{theorem}

Before proving the theorem, we describe it's
consequence for the online learning problem.
We can restrict Theorem~\ref{thm:smallerSteps} for the
case $t=0$,$\R=0$ in which case we get an increasing sequence:
\[
\pot(S_1,0,0) < \pot(S_2,0,0) <\cdots <\pot(S_k,0,0) <
\]
The limit of the strategies $S_k$ as $k \to \infty$ is the well
studied Brownian or Wiener process. We will discuss this connection in Section~\ref{}.

We now go back to proving Theorem~\ref{thm:smallerSteps}. The core of
the proof is a lemma which compares, essentially, the value recursion
when taking one step of size 1 to four steps of size 1/2.


Consider the advesarial strategies $S_k$ and $S_{k+1}$ at a particular
time point $0 \leq t \leq T$ such that $t$ is divisible by
$\deltat=2^{-2k}$ and at a particular location $\R$. Let
$t'=t+\deltat$, and fix a value
function for time , $\pot(t',\R)$ and compare between
two values at $\R,t$. The first value denoted
$\pot_k(t,\R)$ corresponds to $S_k$, and consists of a single random step of $\pm 2^{-k}$. 
The other value $\pot_{k+1}(t,\R)$ corresponds to $S_{k+1}$ and consists of
four random steps of size $\pm 1/2$.

\begin{lemma} \label{lemma:n-strictly-convex}
If $\pot(t',\R)$ as function of $\R$ is in $\SP{4}$, then 
\begin{itemize}
\item $\pot_k(t,\R) < \pot_{k+1}(t,\R)$
\item Both $\pot_k(t,\R)$ and $\pot_{k+1}(t,\R)$ are in $\SP{4}$.
\end{itemize}
\end{lemma}

\proof
Recall the notations $\deltat = 2^{-2k}$ $t' = t+\deltat$ and $s=2^{-k}$.
We can write out explicit expressions for the two values:
\begin{itemize}
\item For strategy $S_0$ the value is
  $$\pot_k(t, \R) = \frac{\pot(t',\R+s)+ \pot(t',\R-s)}{2} $$.
\item For strategy $S_1$ the value is
  $$\pot_{k+1}(t, \R) = \frac{1}{16}
  \paren{\pot(t',\R+2s)+ 4\pot(t',\R+s)+ 6\pot(t',\R)+  4\pot(t',\R-s)+ \pot(t',\R-2s)}
  $$.
\item the difference between the values is
 $$ \pot_{k+1}(t, \R)-\pot_{k}(t, \R) = \frac{1}{16}
  \paren{\pot(t',\R+2s)- 4\pot(t',\R+s)+ 6\pot(t',\R)-
    4\pot(t',\R-s)+ \pot(t',\R-2s)}
  $$
\item Our goal is to show that the RHS is positive. Therefor we can
  divide it by the positive constant  $\frac{2}{3} s^4$, and call the
  resulting function $f$:
  \begin{equation} \label{eqn:divdiff}
  g(\R)=\frac{1}{24s^4} \paren{\pot(t,\R+2s)- 4\pot(t,\R+s)+ 6\pot(t,\R)-
    4\pot(t,\R-s)+ \pot(t,\R-2s)}
  \end{equation}
\end{itemize}

The function $g(\R)$ has a special form called ``divided difference''
that has been extensively studied ~\cite{popoviciu1965certaines,butt2016generalization, de2005divided}.
and is closely related to to derivatives of different orders. Using
this connection and the fact that $\pot(\cdot,\R) \in \SP{4}$ we prove
the following lemma:

The following lemma states that $g(\R)>0$ for all $\R$.
\begin{lemma} \label{lemma:divdiff}
Fix $t>0$ and $s>0$, let $\pot(t,\R) \in \SP{4}$ as a function of $\R$,
and let 
\begin{equation}
g(\R)\doteq\frac{1}{24s^2} \paren{\pot(t,\R+2s)- 4\pot(t,\R+s)+ 6\pot(t,\R)-
    4\pot(t,\R-s)+ \pot(t,\R-2s)}
\end{equation}
then $\forall \R,\; g(\R)>0$ 
\end{lemma}
The proof is given in appendix~\ref{sec:divdiff}

We conclude that if $\pot(t',\R)$ has a strictly positive fourth
derivative then $\pot_{k+1}(t,\R) > \pot_{k}(t,\R)$ for all $\R$, proving
the first part of the lemma.

The second part of the lemma follows from the fact that
both $\pot_{k+1}(t,\R)$ and $\pot_{k}(t,\R)$ are convex combinations of
$\pot(t,\R)$ and therefor retain their continuity and convexity properties.

\qed

\proof  of Theorem~\ref{thm:smallerSteps} \\
The proof is by double induction over $k$ and over $t$.
For a fixed $k$ we take a finite backward induction over
$t=T-2^{-2k},T-2 \times 2^{-2k},T-3 \times 2^{-2k},\cdots,0$.
Our inductive claims are that $\pot_{k+1}(t,\R) > \pot_{k}(t,\R)$ and
$\pot_{k+1}(t,\R)$,$\pot_{k}(t,\R)$ are continuous, strongly convex and
have a strongly positive fourth derivative. That these claims carry over
from $t=T-i \times 2^{-2k}$ to  $t=T-(i+1) \times 2^{-2k}$ follows
directly from Lemma~\ref{lemma:n-strictly-convex}.

The theorem follows by forward induction on $k$.

\qed


\iffalse
Next, we consider the discrete time version of $\learnerM^2$:
(Eqn~\ref{eqn:learner-strat-2})
\begin{eqnarray} \label{eqn:learner-strat-2c}
  \learnerM^{2d}(t_{i},\R) =  \frac{1}{Z^{2d}}
  \left. \frac{\partial}{\partial r} \right|_{r=\R}
  \pot(t_{i}+s_{i}^2,r)
  \\
  \mbox{ where } Z^{2d} = \E{\R \sim
  \state(t_i)}{\left. \frac{\partial}{\partial r} \right|_{r=\R} \pot(t_{i}+s_{i}^2,r)} \nonumber
\end{eqnarray}
\fi

\section{Brownian motion and min/max strategies}

In the previous section we described a sequence of adversarial
strategies $S_1,S_2,\ldots$ and a learner strategy such that ....

described a sequence of adversarial
strategies

It is well known that the limit of random walks where $s \to 0$ and
$\deltat=s^2$ is the the Brownian or Wiener process
(see~\cite{kac1947random}).

An alternative characterization of Brownian Process is
$$ \P{}{X_{t+\deltat}=x_1 | X_t=x_0}=e^{-\frac{(x_1-x_0)^2}{2 \deltat}}$$

The backwards recursion that
defines the value function is the celebrated Backwrds Kolmogorov
Equation with no drift and unit variance
\begin{equation} \label{eqn:Kolmogorov}
  \frac{\partial}{\partial t} \pot(t,\R)
  + \frac{1}{2} \frac{\partial^2}{\partial \R^2} \pot(t,\R)=0
\end{equation}
Given a final value function with a strictly positive fourth
derivative we can use Equation~(\ref{eqn:Kolmogorov}) to compute the
value function for all $0 \leq t \leq T$. We will do so in he next section.

\section{Stable potential functions and anytime strategies} \label{sec:stable}

The potential functions, $\pot(t,\R)$ is a solution of PDE~(\ref
{eqn:Kolmogorov}):
\begin{equation} 
  \frac{\partial}{\partial t} \pot(t,\R)
  + \frac{1}{2} \frac{\partial^2}{\partial r^2} \pot(t,\R)=0
\end{equation}
under a boundary condition $\pot(T,\R)=\finalPot(\R)$, which we assume
is in $\SP{4}$

So far, we assumed that the game horizon $T$ is known in advance. We
now show two value functions where knowledge of the horizon is not
required. Specifically, we call a value function $\pot(t,\R)$
{\em self consistent} if it is defined for all $t>0$ and if for any
$0<t<T$, setting $\phi(T,\R)$ as the final potential and solving for
the Kolmogorov Backward Equation yields $\phi(t,\R)$ regarless of the
time horizon $T$. 

We consider two solutions to the PDE, the exponential potential and
the NormalHedge potential. We give the form of the potential function
that satisfies Kolmogorov Equation~\ref{eqn:Kolmogorov}, and derive
the regret bound corresponding to it.

{\bf The exponential potential function} which corresponds to exponential
  weights algorithm corresponds to the following equation
\[
    \pot_{\mbox{exp}}(\R,t) = e^{\sqrt{2} \eta \R - \eta^2 t}
  \]
  Where $\eta>0$ is the learning rate parameter.
  
Given $\epsilon$ we choose $\eta = \sqrt{\frac{\ln (1/\epsilon)}{t}}$
we get the regret bound that holds for any $t>0$
  \begin{equation}
    \R_\epsilon \leq \sqrt{2 t \ln \frac{1}{\epsilon}}
  \end{equation}
Note that the algorithm depends on the choice of $\epsilon$, in other
words, the bound does {\em not} hold for all values of $\epsilon$ at
the same time.

{\bf The NormalHedge value} is
\begin{equation} \label{eqn:NormalHedge}
  \pot_{\mbox{NH}}(\R,t) = \begin{cases}
    \frac{1}{\sqrt{t+\nu}}\exp\left(\frac{\R^2}{2(t+\nu)}\right)
    & \mbox{if } \R \geq 0  \\
  \frac{1}{\sqrt{t+\nu}} & \mbox{if } \R <0
  \end{cases}
\end{equation}
Where $\nu>0$ is a small constant. The function $\pot_{\mbox{NH}}(\R,t)$,
restricted to $\R\geq 0$ is in $\SP{4}$ and is a constant for $\R \leq 0$.

The regret bound we get is:
\begin{equation}
\R_\epsilon \leq \sqrt{(t+\nu) \left( \ln (t+\nu) + 2 \ln \frac{1}{\epsilon}\right)}
\end{equation}
This bound is slightly larger than the bound for exponential weights,
however, the NormalHedge bound holds simultanuously for all
$\epsilon>0$ and the algorithm requires no tuning.

\section{The continuous time game and bounds for easy sequences} \label{sec:easy}
In Section~\ref{sec:discrete} we have shown that the integer time game
has a natural extension to a setting where $\deltat_i = s_i^2$. We
also demonstrated sequences of adversarial strategies
$S_1,S_2,\ldots$ such that $\sup_{k \to \infty} {\lowerpot}_k(0,\R) = $

We characterized the optimal adversarial strategy for the discrete
time game (Section~\ref{sec:discrete-Time-Game}), which corresponds
to the adversary choosing the loss to be $s_i$ or $-s_i$ with equal
probabilities. A natural question at this point is to characterize the
regret when the adversary is not optimal, or the sequences are ``easy''.

To see that such an improvement is possible, consider the following
{\em constant} adversary. This adversary associates the same loss to
all experts on iteration $i$, formally, $\adversM(i,\R) = l$. In this
case the average loss is also equal to $l$, $\ell(i)=l$ which means
that all of the instantaneous regrets are $r=l-\ell(t_i) = 0$, which,
in turn, implies that $\state(i) = \state(i+1)$. As the state did not
change, it makes sense to set $t_{i+1}=t_i$, rather than
$t_{i+1}=t_i+s_i^2$.

We observe two extremes for the adversarial behaviour. The constant
adversary described above for which $t_{i+1} = t_i$, and the random walk adversary described
earlier, in which each expert is split into two, one half with loss
$-s_i$ and the other with loss $+s_i$. In which case $t_{i+1} =
t_i+s_i^2$ which is the maximal increase in $t$ that the adversary can
guarantee. The analysis below shows that these are two extremes on a
spectrum and that intermediate cases can be characterized using a
variance-like quantity.

We define a variant of the discrete time game
(\ref{sec:discrete-Time-Game}) For concreteness we include the
learner's strategy, which is the limit of the strategy in the discrete
game when $s_i \to 0$.


\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
Set $t_1=0$ \\
Fix maximal step $0<s<1$ \\
On iteration $i=1,2,\ldots$

\begin{enumerate}
\item  If $t_i=T$ the game terminates.
\item Given $t_{i}$, the learner chooses a distribution
  $\learnerM(i)$ over $\reals$:
  \begin{equation} \label{eqn:learner-strat-cc}
  \learnerM^{cc}(t,\R) =  \frac{1}{Z^{cc}}
  \left. \frac{\partial}{\partial r} \right|_{r=\R} \pot(t,r)
  \mbox{ where } Z^{cc} = \E{\R \sim \state(t_i)}{\left. \frac{\partial}{\partial r} \right|_{r=\R} \pot(t,r)}
\end{equation}

\item The adversay chooses a {\em step size} $0<s_i\leq s$ and a mapping from $\reals$ to distributions
  over $[-s_i,+s_i]$: $\adversM(t): \reals \to \Delta^{[-s_i,+s_i]}$
\item The aggregate loss is calculated:
  \begin{equation} \label{eqn:ell-discrete}
    \ell(t_i)=\E{\R \sim \state(t_i)}{\learnerM^{cc}(t_i,\R)
      \Bias(t_i,\R)},\;\mbox{ where } \Bias(t_i,\R) \doteq \E{y \sim \adversM(t_i,\R)}{y}
  \end{equation}
  the aggregate loss is restricted to $|\ell(t_i)| \leq c s_i^2$.
\item  Increment $t_{i+1} = t_{i} + \deltat_i$ where
\begin{equation} \label{eqn:deltat}
  \deltat_i=
  \E{\R \sim \state(t_i)}{H(t_i,\R) \;\; \E{y \sim \adversM(t_i,\R)}{(y-\ell(t_i))^2}}
\end{equation}
Where
\begin{equation}
 H(t_i,R)=\frac{1}{Z^H} \left. \frac{\partial^2}{\partial r^2} \right|_{r=\R} \pot(t_i,r)
  \mbox{ and } Z^H = \E{\R \sim \state(t_i)}{\left. \frac{\partial^2}{\partial r^2} \right|_{r=\R} \pot(t_i,r)}
\end{equation}

\item The state is updated.
  $$\state(t_{i+1}) = \E{\R \sim \state(t_{i})}{\adversM(t_i)(\R)\oplus (\R-\ell(t_i))}
  $$
\end{enumerate}
\end{minipage}}
\caption{The continuous time game and learner strategy\label{sec:contin-Time-Game}}
\end{figure}

Our characterization applies to the limit where the $s_i$ are small. Formally, we define
\begin{definition}
We say that an instance of the discrete time game is
$(n,s,\tau)$-bounded if it consists of $n$ iterations and $\forall\;\; 0<i\leq n,\;\; s_i < s$ and $\sum_{j=1}^n s_j^2=\tau$
\end{definition}

Note that $\tau>t_n$ and that $\tau$ depends only on the ranges $s_i$
while $t_n$ depends on the variance. $t_n = T$ 
is the dominant term in the regret bound, while $\tau$ controls the
error term.

\newpage

\begin{theorem} \label{thm:variancebound} Let $\pot \in \SP{\infty}$
  be a potential function that satisfies the Kolmogorov backward
  equation~(\ref{eqn:Kolmogorov}).
  Fix the total time $\tau$ and let $G_n$ be an $(n,
  \sqrt{\frac{\tau}{n}},\tau)$-bounded game. Let $n \to \infty$.

Then 
$$\score(\state(\tau)) \leq \score(\state(0))+O\paren{\frac{1}{\sqrt{n}}}$$
\end{theorem}

The proof is given in appendix~\ref{appendix:ProofOfVarianceBound}

If we define

\begin{equation} \label{eqn:Vn}
  V_n = t_n = \sum_{i=1}^n \deltat_i= 
  \sum_{i=1}^n \E{\R \sim \state(t_i)}{\E{y \sim \adversM(t_i,\R)}{H(t_i,\R) ((y-\ell(t_i))^2)}}
\end{equation}

We can use $V_n$ instead of $T$ giving us a variancee based bound.

  
%    \E{\R \sim \state(i)}{ \E{y \sim
%      \adversM(i)(\R)}{\pot(i,\R+y-\ell(i))}}


\bibliographystyle{plain}
\bibliography{ref.bib,bib.bib}

\appendix
\section{Proof of Theorem~\ref{thm:simulBoundAveragePot} \label{proof:simulBoundAveragePot}}
\proof
  \begin{itemize}
  \item
  {\bf $\state$ satisfies a simultanous bound for $B$ if it satisfies an
  average potential bound for $\pot = B^{-1}$}\\
Assume by contradiction that $\state$ does not satisfy the simultanous bound. In
other words there exists $a \in \reals$ such that
$\P{\R \sim \state}{\R > a} > B(a)$. From Markov inequality and the fact
that $\phi$ is non decreasing we get
\[
  \E{\R \sim \state}{\pot(\R)} \geq \phi(a) \P{\R \sim \state}{\R > a} >
  \phi(a) B(a) = \frac{B(a)}{B(a)}=1
\]
but $ \E{\R \sim \state}{\pot(\R)} >1$ contradicts the average potential
assumption for the potential $\phi(\R) = B(\R)^{-1}$
\item
{\bf $\state$ satisfies an
  average potential bound for $\pot = B^{-1}$ if it satisfies a simultanous bound for $B$}\\
As $\phi$ is a non-decreasing function, and assuming $\R,\R'$ are drawn
independently at random according to $\state$:
\begin{eqnarray}
  \E{\R \sim \state}{\pot(\R)} & = & \E{\R \sim \state}{\pot(\R)
                                  \P{\R' \sim \state}{\phi(R') \geq \phi(\R)}} \\
                            & \leq & \E{\R \sim \state}{\pot(\R)
                                     \P{\R' \sim \state}{R' \geq \R}} \\
                            & < & \E{\R \sim \state}{\pot(\R) B(\R)} \\
                            & = & \E{\R \sim \state}{\frac{B(\R)}{B(\R)}}
                                  = \E{\R \sim \state}{1} = 1
\end{eqnarray}
\end{itemize}
\qed

\section{Divided differences of a function} \label{sec:divdiff}


the lhs has the form
A function $\finalPot$ that satisfies
inequality~\ref{eqn:4thOrderConvex} is said to be {\em 4'th order convex}
(see details in in~\cite{butt2016generalization}).


Following\cite{butt2016generalization} we give a brief review of
divided differences and of $n$-convexity.

Let $f:[a,b] \to \reals$ be a function from the segment $[a,b]$ to the
reals.

\begin{definition}[$n$'th order divided difference of a function]
  The $n$'th order divided different of a function $f:[a,b] \to
  \reals$ at mutually distinct and ordered points $a \leq x_0 < x_1
  < \cdots < x_n \leq b$
  defined recursively by
  \[ [x_i; f] = f(x_i), \; i \in 0,\ldots n,\]
  \[ [x_0,\ldots,x_n;f] =
    \frac{[x_1,\ldots,x_n;f]-[x_0,\ldots,x_{n-1};f]}{x_n-x_0} \]
\end{definition}

\begin{definition}[$n$-convexity]
 A function $f:[a,b] \to \reals$ is said to be $n$-convex  $n \geq 0$
 if and only if for all choices of $n+1$ distinct points: $a \leq x_0 < x_1
  < \cdots < x_n \leq b$, $[x_0,\ldots,x_n;f]\geq 0$ holds.
\end{definition}
$n$-convexity is has a close connection to the sign of $f^{(n)}$ - the $n$'th
derivative of $f$, this connection was proved in 1965 by
popoviciu~\cite{popoviciu1965certaines}.
\begin{theorem} \label{thm:popo}
If $f^{(n)}$ exists then f is $n$-convex if and only if $f^{(n)}\geq 0$.
\end{theorem}

The next lemma states that the function $g(\R)>0$ as defined in
Equation~(\ref{eqn:divdiff}).

\proof {\bf of Lemma~(\ref{lemma:divdiff})

Fix $t$ and define $f(x) = \pot(t,x)$.
Let $(x_0,x_1,x_2,x_3,x_4)=(\R-2 s,\R-s,\R,\R+s,\R+2s)$

Using this notation we can rewrite $g(\R)$ in the form
\begin{equation}
  h(x_0,x_1,x_2,x_3,x_4) =  \frac{1}{24s^4} \paren{f(x_4)- 4f(x_3)+ 6f(x_2)-
    4f(x_1)+ f(x_0)}
\end{equation}
  
k

Is the 4-th order divided difference of $\pot(t,\cdot)$


\begin{enumerate}
\item
$$[x_i;f] = f(x_i)$$
\item
  $$[x_i,x_{i+1};f]=\frac{f(x_{i+1})-f(x_i)}{s}$$
\item
  $$[x_i,x_{i+1},x_{i+2};f] =
  \frac{\frac{f(x_{i+2})-f(x_{i+1})}{s}-\frac{f(x_{i+1})-f(x_i)}{s}}{2s}
  =\frac{f(x_{i+2})-2f(x_{i+1})+f(x_i)}{2 s^2}
  $$
\item
  \begin{eqnarray*}
    [x_i,x_{i+1},x_{i+2},x_{i+3};f]& = &
    \frac{\frac{f(x_{i+3})-2f(x_{i+2})+f(x_{i+1})}{2
    s^2}-\frac{f(x_{i+2})-2f(x_{i+1})+f(x_i)}{2
    s^2}}{3s}\\
    &=& \frac{f(x_{i+3}) -3f(x_{i+2})+3f(x_{i+1})-f(x_i)   }{6 s^3}
  \end{eqnarray*}
\item
  \begin{eqnarray*}
    [x_i,x_{i+1},x_{i+2},x_{i+3},x_{i+4};f]& = &
    \frac{\frac{f(x_{i+4}) -3f(x_{i+3})+3f(x_{i+2})-f(x_{i+1})   }{6 s^3}
    - \frac{f(x_{i+3}) -3f(x_{i+2})+3f(x_{i+1})-f(x_i)   }{6 s^3}}
    {4s}\\
    &=& \frac{f(x_{i+4})-4f(x_{i+3})+6f(x_{i+2})-4f(x_{i+1})+f(x_i)}{24s^4}
  \end{eqnarray*}
\end{enumerate}

\qed
\section{Proof of Theorem~\ref{thm:variancebound}}
\label{appendix:ProofOfVarianceBound}
We start with two technical lemmas
\begin{lemma} \label{lemma:infiniteexpectations}
Let $f(x) \in \SP{2}$, i.e. $f(x), f'(x),f''(x) >0$ for all $x \in
\reals$, let $h(x)$ be a uniformly bounded function: $\forall x,\;\; |h(x)|<1$.
Let $\state$ be a distribution over $\reals$.
If $\E{x \sim \state}{f(x)}$ is well-defined (and finite) , then 
$\E{x \sim \state}{h(x) f'(x)}$ is well defined (and finite) as well.
\end{lemma}
\proof
Assume by contradiction that $\E{x \sim \state}{h(x) f'(x)}$ is
undefined. Define $h^+(x) = \max(0,h(x))$.
As $f'(x)>0$, this implies that either $\E{x \sim \state}{h^+(x)
  f'(x)}=\infty$ or $\E{x \sim \state}{(-h)^+(x) f'(x)}=\infty$ (or both). 

Assue wlog that $\E{x \sim \state}{h^+(x) f'(x)}=\infty$. As
$f'(x)>0$ and $0 \leq h^+(x) \leq 1$ we get that $\E{x \sim
  \state}{f'(x)}=\infty$.
As $f(x+1) \geq f'(x)$ we get that $\E{x \sim
  \state}{f(x)}=\infty$ which is a contradiction.
\qed


\newcommand{\Dx}{\Delta x}
\newcommand{\Dy}{\Delta y}
\begin{lemma} \label{lemma:Taylor2D}
Let $f(x,y)$ be a differentiable function with continuous derivatives
up to degree three. Then
\begin{eqnarray}
  &&f(x_0+\Dx,y_0+\Dy) = f(x_0,y_0)
  + \atI{\frac{\partial}{\partial x}} \Dx 
  + \atI{\frac{\partial}{\partial y}} \Dy \\
  &+&\frac{1}{2} \atI{\frac{\partial^2}{\partial x^2}} \Dx^2
      +\atI{\frac{\partial^2}{\partial x\partial y}} \Dx\Dy
      +\frac{1}{2} \atI{\frac{\partial^2}{\partial y^2}} \Dy^2\\
  &+&\frac{1}{6} \atII{\frac{\partial^3}{\partial x^3}} \Dx^3
      +\frac{1}{2} \atII{\frac{\partial^3}{\partial x^2 \partial y}} \Dx^2\Dy\\
  &&+ \frac{1}{2} \atII{\frac{\partial^3}{\partial x \partial y^2}} \Dx\Dy^2
    + \frac{1}{6} \atII{\frac{\partial^3}{\partial y^3}} \Dy^3
\end{eqnarray}
for some $0\leq t \leq 1$.
\end{lemma}
\proof {\em of Lemma~\ref{lemma:Taylor2D}} 
Let $F:[0,1] \to \reals$ be defined as  $F(t)=f(x(t),y(t))$ where
$x(t) = x_0+t\Dx$ and $y(t)=y_0+t\Dy$. Then $F(0)=f(x_0,y_0)$ and
$F(1)=f(x_0+\Dx,y_0+\Dy)$. It is easy to verify that
$$ \frac{d}{dt}F(t)
=\frac{\partial}{\partial x} f(x(t),y(t))\Dx
+ \frac{\partial}{\partial y} f(x(t),y(t))\Dy
$$
and that in general:
\begin{equation} \label{eqn:d.dn.F}
\frac{d^n}{d t^n} F(t) = \sum_{m=1}^n {n \choose m}
\frac{\partial^n}{\partial x^m \partial y^{n-m}} f(x_0+t \Dx,y_0+t\Dy)
\Dx^m \Dy^{n-m}
\end{equation}
As $f$ has partial derivatives up to degree 3, so does $F$. Using the
Taylor expansion of $F$ and the intermediate point theorem we get that
\begin{equation} \label{eqn:Taylor.F}
  f(x_0+\Dx,y_0+\Dy) = F(1) = F(0)+\frac{d}{dt}F(0)
  +\frac{1}{2}\frac{d^2}{dt^2}F(0)
  +\frac{1}{6}\frac{d^3}{dt^3}F(t')
\end{equation}
Where $0 \leq t' \leq 1$. Using Eqn~(\ref{eqn:d.dn.F}) to expand each
term in Eqn.~(\ref{eqn:Taylor.F}) completes the proof.
\qed


\proof {\em of Theorem~\ref{thm:variancebound}}\\
We prove the claim by an upper bound on the increase of potential that holds for any iteration $1 \leq i \leq n$:
\begin{equation} \label{proof:onestep}
\score(\state(t_{i+1})) \leq \score(\state(t_i)) + a s_i^3 \mbox{ for some constant } a>0
\end{equation}
Summing inequality~(\ref{proof:onestep}) over all iterations we get that 
\begin{equation} \label{proof:allsteps}
\score(\state(T)) \leq \score(\state(0)) + c \sum_{i=1}^n s_i^3 \leq 
\score(\state(0)) + a s \sum_{i=1}^n s_i^2 = 
\score(\state(0)) + a s T
\end{equation}
From which the statement of the theorem follows.

We now prove inequality~(\ref{proof:onestep}). 
We use the notation $r=y -\ell(i)$ to denote the instantaneous regret at iteration $i$. 


Applying Lemma~\ref{lemma:Taylor2D} to
$\pot(t_{i+1},\R_{i+1})=\pot(t_i+\deltat_i,\R_i+r_i)$  we get
\begin{eqnarray} 
    \pot(t_i+\deltat_i,\R_i+r_i) & =&  
    \pot(t_i,\R_i)\\
    &+&\at{\frac{\partial}{\partial \rho}} r_i \\
    &+&\at{\frac{\partial}{\partial \tau}}  \deltat_i \\
    &+& \frac{1}{2} \at{\frac{\partial^2}{\partial \rho^2}} r_i^2 \\
    &+& \at{\frac{\partial^2}{\partial r \partial \tau}} r_i \deltat_i \label{term:Taylor_rdt}\\
    &+& \frac{1}{2} \at{\frac{\partial^2}{\partial \tau^2}} \deltat_i^2 \label{term:Taylor_dtsquare}\\
    &+& \frac{1}{6} \att{\frac{\partial^3}{\partial \rho^3}} r_i^3 \label{term:Taylor_r3}\\
    &+& \frac{1}{2} \att{\frac{\partial^3}{\partial \rho^2 \partial \tau}} r_i^2\deltat_i \label{term:Taylor_r2t}\\
    &+& \frac{1}{2} \att{\frac{\partial^3}{\partial \rho \partial \tau^2}} r_i\deltat_i^2 \label{term:Taylor_rt2}\\
    &+& \frac{1}{6} \att{\frac{\partial^3}{\partial \tau^3}} \deltat_i^3 \label{term:Taylor_t3}
\end{eqnarray}
for some $0 \leq g \leq 1$.

By assumption $\pot$ satisfies the Kolmogorov backward equation:
\begin{equation*} 
  \frac{\partial}{\partial \tau} \pot(\tau,\rho)
  = -\frac{1}{2} \frac{\partial^2}{\partial r^2} \pot(\tau,\rho)
\end{equation*}
Combining this equation with the exchangability of the order of
partial derivative (Clairiaut's Theorem) we can substitute all
partial derivatives with respect to $\tau$ with partial derivatives
with respect to $\rho$ using the following equation.
\[
  \frac{\partial^{n+m}}{\partial \rho^n \partial \tau^m} \pot(\tau,\rho)=
  (-1)^m \frac{\partial^{n+2m}}{\partial \rho^{n+2m}} \pot(\tau,\rho)
\]
Which yields
\begin{eqnarray}
      \pot(t_i+\deltat_i,\R_i+r_i) & =&  
    \pot(t_i,\R_i)\\
    &+& \at{\frac{\partial}{\partial \rho}} r_i \label{term:coll1}\\
    &+& \at{\frac{\partial^2}{\partial \rho^2}} \paren{\frac{r_i^2}{2}-\deltat_i} \label{term:coll2}\\
    &-& \at{\frac{\partial^3}{\partial \rho^3}} r_i \deltat_i \label{term:coll3}\\
    &+& \frac{1}{2} \at{\frac{\partial^4}{\partial \rho^4}} \deltat_i^2 \label{term:coll4}\\
    &+& \frac{1}{6} \att{\frac{\partial^3}{\partial \rho^3}} r_i^3 \label{term:coll5}\\
    &-& \frac{1}{2} \att{\frac{\partial^4}{\partial \rho^4}} r_i^2\deltat_i \label{term:coll6}\\
    &+& \frac{1}{2} \att{\frac{\partial^5}{\partial \rho^5}} r_i\deltat_i^2 \label{term:coll7}\\
    &-& \frac{1}{6} \att{\frac{\partial^6}{\partial \rho^6}} \deltat_i^3 \label{term:coll8}
\end{eqnarray}

  From the assumption that the game is $(n,s,T)$-bounded we get that 
  \begin{enumerate}
  \item $|r_i| \leq s_i +c s_i^2 \leq 2 s_i$
  \item $\deltat_i \leq s_i^2 \leq s^2$
    % \item $\sum_i \deltat_i=T$
  \end{enumerate}

  given these inequalities we can rewrite the second factor in each
  term as follows, where $|h_i(\cdot)|\leq 1$
  \begin{itemize}
  \item {\bf For~(\ref{term:coll1}):}
    $r_i=2s_i\frac{r_i}{2s_i}=2s_ih_1(r_i)$.
  \item {\bf For~(\ref{term:coll2}):}
    $r_i^2 - \frac{1}{2}\deltat_i = 4s_i^2\frac{r_i^2 -
      \frac{1}{2}\deltat_i}{4s_i^2} = 4s_i^2 h_2(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll3}):} $r_i \deltat_i = 2s_i^3
    \frac{r_i \deltat_i}{2s_i^3} = 2s_i^3 h_3(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll4}):} $\deltat_i^2 =
    s_i^4\frac{\deltat_i^2}{s_i^4} = s_i^3 h_4(\deltat_i)$
  \item {\bf For~(\ref{term:coll5}):} $r_i^3 = 8s_i^3
    \frac{r_i^3}{8s_i^3} = 8s_i^3 h_5(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll6}):} $r_i^2 \deltat_i = 4s_i^4
    \frac{r_i^2 \deltat_i}{4s_i^4} = 4s_i^3 h_6(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll7}):} $r_i \deltat_i^2 = 2s_i^5
    \frac{r_i \deltat_i^2}{2s_i^5}$
  \item {\bf For~(\ref{term:coll8}):} $\deltat_i^3 = s_i^6 \frac{\deltat_i^3}{s_i^6}$
\end{itemize}
  We therefor get the simplified equation
  
  \begin{eqnarray*} 
     \pot(t_i+\deltat_i,\R_i+r_i) & =&  \pot(t,\R)+\at{\frac{\partial}{\partial r}} r
    + \at{\frac{\partial}{\partial t}} \deltat \\
                                  &+& 
                                      \frac{1}{2}  \at{\frac{\partial^2}{\partial r^2}} r^2\\
                                  &+& \at{\frac{\partial^2}{\partial r \partial t}} r_i \deltat_i \label{term:Taylor_collected_rdt}\\
                                  &+& \frac{1}{6} \at{\frac{\partial^3}{\partial r^3}} r_i^3 \label{term:Taylor_collected_r3}
                                      + O(s^4)
\end{eqnarray*}

and therefor
  \begin{eqnarray} 
     \pot(t_i+\deltat_i,\R+r) &=& \pot(t_i,\R) +
                                  \at{\frac{\partial}{\partial r}} r
                                  \nonumber \\
    &+& \at{\frac{\partial^2}{\partial r^2}} (r^2 - \deltat_i) +
        O(s^3) \label{eqn:Taylor}
\end{eqnarray}

Our next step is to consider the expected value of~(\ref{eqn:Taylor}) wrt $\R \sim \state(t_i)$,
$y \sim \adversM(t_i,\R)$ for an arbitrary adversarial strategy
$\adversM$.

We will show that the expected potential does not increase:
\begin{equation} \label{eqn:deltatislargeenough}
     \E{\R \sim \state(t_i)}{ \E{y \sim \adversM(t_i,\R)}{\pot(t_i+\deltat_i,\R+y-\ell(t_i))}} \leq \E{\R \sim \state(t_i)}{\pot(t_i,\R)}
\end{equation}

Plugging Eqn~(\ref{eqn:Taylor}) into the LHS of
Eqn~(\ref{eqn:deltatislargeenough}) we get
\begin{eqnarray}
  \lefteqn{\E{\R \sim \state(t_i)}{\E{y \sim \adversM(t_i,\R)}{\pot(t_i+\deltat_i,\R+y-\ell(t_i))}}} \\
  &=& \E{\R \sim \state(t_i)}{\pot(t_i,\R)} \label{eqn:contin0}\\
  &+& \E{\R \sim \state(t_i)}{\E{y \sim \adversM(t_i,\R)}{\at{\frac{\partial}{\partial r}} (y-\ell(t_i))}} \label{eqn:contin1}\\
  &+& \E{\R \sim \state(t_i)}{\E{y \sim
      \adversM(t_i,\R)}{\at{\frac{\partial^2}{\partial r^2}}
      ((y-\ell(t_i))^2 - \deltat_i)}}
  \label{eqn:contin2}\\
  &+& O(s^3) \label{eqn:contin3}
\end{eqnarray}
Some care is needed here. we need to show that the expected value
are all finite. We assume that the expected potential
(Eqn~({eqn:contin0}) is finite. Using
Lemma~\ref{lemma:infiniteexpectations} this implies that the expected
value of higher derivatives of $\frac{\partial}{\partial \R} \pot(\R)$
are also finite.\footnote{I need to clean this up and find an argument
  that the expected value for mixed derivatives is also finite.}


To prove inequality~(\ref{proof:onestep}), we need to show that the
terms~\ref{eqn:contin1} and \ref{eqn:contin2} are smaller or equal to
zero.
~\\~\\~\\
{\bf Term~(\ref{eqn:contin1}) is equal to zero:}\\
As $\ell(t_i)$ is a constant
relative to $\R$ and $y$, and $\at{\frac{\partial}{\partial r}}$ is a
constant with respect to $y$ we can rewrite~(\ref{eqn:contin1}) as
\begin{equation} \label{eqnterm1.1}
  \E{\R \sim \state(t_i)}{\at{\frac{\partial}{\partial r}}
    \E{y \sim \adversM(t_i,\R)}{y} }
- \ell(t_i) \E{\R \sim \state(t_i)}{\at{\frac{\partial}{\partial r}}}
\end{equation}

Combining the definitions of $\ell(t)$~(\ref{eqn:ell-discrete}) and~
and the learner's strategy
$\learnerM^{cc}$~(\ref{eqn:learner-strat-cc}) we get that
\begin{eqnarray}
\ell(t_i) &=& \E{\R \sim \state{t_i}}{\frac{1}{Z}
              \at{\frac{\partial}{\partial r}}
              \E{y \sim \adversM(i,\R)}{y}} \mbox{ where }
              Z=\E{\R \sim \state{t_i}}{\frac{1}{Z}
              \at{\frac{\partial}{\partial r}}}
              \label{eqnterm1.2}
\end{eqnarray}

Plugging~(\ref{eqnterm1.2}) into (\ref{eqnterm1.1}) and recalling the
requirement that $\ell(t_i)<\infty$ we find that
term~(\ref{eqn:contin1}) is equal to zero.


~\\~\\~\\
{\bf Term~(\ref{eqn:contin2}) is equal to zero:}\\
As $\deltat_i$ is a constant relative to $y$, we can take it
outside the expectation and plug in the definition of $\deltat_i$ (\ref{eqn:deltat})
\begin{equation} \label{eqn:term2.1}
  \E{\R \sim \state(t_i)}{\E{y \sim
      \adversM(t_i,\R)}{\adversM(t_i,\R)}{\at{\frac{\partial^2}{\partial r^2}}
      (y-\ell(t_i))^2} - \deltat_i}=
  \deltat_i - \deltat_i =0
\end{equation}
Where $G(t_i,\R)$ is defined in Equation~(\ref{eqn:SecondOrderDerivative})
We find that (\ref{eqn:contin2}) is zero.

Finally (\ref{eqn:contin3}) is negligible relative to the other terms
as $s \to 0$.
\qed 

\end{document}

