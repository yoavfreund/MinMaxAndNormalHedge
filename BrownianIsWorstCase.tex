\documentclass{article}[12pt]
\usepackage{fullpage}
% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}

\input{macros}

\title{Potential-based hedging algorithms}
\author{Yoav Freund}
\begin{document}

\maketitle
\begin{abstract}
  We study regret-minimizing online algorithms based on potential
functions. First, we show that any algorithm with a regret bound that
holds for any $\epsilon$ is equivalent to a potential minimizing
algorithm and vice versa. Second we should a min-max learning
algorithm for known horizon. We show a regret bound that is close to
optimal when the horizon is not known. Finally we give an algorithm
with second order bounds that characterize easy sequences.
\end{abstract}

\section{Introduction}
Online prediction with expert advise has been studied extensively over
the years and the number of publications in the area is vast (see
e.g.~\cite{vovk1990aggregating, feder1992universal,
  littlestone1994weighted, cesa1997use, cesa2006prediction}.

Here we focus on a simple variant of online prediction with expert
advice called {\em the decision-theoretic online learning game}
(DTOL)~\cite{freund1997decision}, we  consider the 
  signed version of this game.

DTOL (Figure~\ref{fig:DTOL}) is a repeated zero sum game between a {\em learner} and an {\em
  adversary}. The adversary controls the losses of $N$ actions, while
the learner controls a distribution over the actions.

\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
For $i=1,\ldots,T$
\begin{enumerate}
    \item The learner chooses a weight function $w_j^i$ over the
      actions $j \in \{1,\ldots,N\}$. 
    \item The adversary chooses an {\em instantaneous loss} for each
      of the $N$ actions: \\
      $l_j^i \in [-1,+1]$ for $j \in \{1,\ldots,N\}$.
    \item The {\em cumulative loss of action $j$} 
    is  $L^i_j = \sum_{s=1}^i l_j^s$. 
    \item The learner incurs an {\em instantanous average loss} defined as
      $\ell^i = \frac{\sum_{j=1}^N w_j^i l_j^i}{\sum_{j=1}^N w_j^i}$
    \item The {\em cumulative loss of the learner} is
      $L_\ell^i = \sum_{s=1}^i \ell^s$
    \item The {\em cumulative regret} of the learner with respect to
      action $j$ is
      $\R_j^i = L_\ell^i -L_j^i $.
\end{enumerate}
\end{minipage}}
\caption{Decision theoretic online learning \label{fig:DTOL}}
\end{figure}

The goal of the learner (in the percentile version of the game) is to
perform almost as well as $k$ best actions. Specifically, we sort the
regrets in decreasing order
$\R_1^i \geq \R_2^i \geq \cdots \geq \R_k^i \geq \cdots$ and define
$\R_k^i$ to as the regret relative to the $\epsilon=k/M$ top
percentile, denote $\R_\epsilon^i$. Our goal is to find algorithms
that guarantee small upper bounds on $\R_\epsilon^T$. Known bounds
have the form $c \sqrt{T \ln {1/\epsilon}}$, but the algorithm has to
be tuned based on prior knowledge of $\epsilon$. We seek algorithms
with regret bounds that hold {\em simultanously} for all values of
$\epsilon$. In other words algorithms that do not need to know
$\epsilon$ or $t$ ahead of time. The following definition formalizes
the concept of simultanous bounds:
\begin{definition}[Simultanous regret
  bound] \label{def:unif-regret-bound} Let $G: \reals \to [0,1]$ be a
  non-increasing function which maps regret bounds to probabilities.
  A distribution over regrets $\state$ is simultanously bound by $G$ if
  \[
    \forall r \in \reals \;\; \P{\rho \sim \state(T)}{\rho \geq r} \leq G(r)
  \]
\end{definition}


A potential function is an increasing function
$\pot:\reals \to \reals$. Potential based learing algorithm are
designed to bound the the average potential
$\E{\R \sim \state}{\pot(\R)}$. Potential functions have long been
used to design and analyze online learning algorithms. The novelty 
here is that we consider minimizing the average potential as a goal in itself.
\begin{definition}[Average potential bound] \label{def:aver-potential-bound}
  A distribution over he reals $\state$ satisfies the average
  potential function $\pot$ if
  $$\E{\R \sim \state}{\pot(\R)} \leq 1$$
  Where $\pot: \reals \to \reals^+$ is a non decreasing function. 
\end{definition}

The next theorem identifies a one to one relationship between
simultanous bounds and average potential bounds. 
\begin{theorem}\label{thm:simulBoundAveragePot}
 A distribution $\state$ is simultanously bounded by $B$ if and only
 if it satisfies the average potential bound with $\phi(\R) = B(\R)^{-1}$
\end{theorem}
The proof of the theorem is in Appendix~\ref{proof:simulBoundAveragePot}.

Simultanous regret bounds are more intuitive than potential
functions. On the other hand potent, but average potential
bounds lend themselves to analysis and optimization. In 



given the potential function at the end of the game $\pot(T,\R)$ and
the strategies used by the learner and the adversary, we can define a
potential $\pot(T-1,\R)$ such that the average potentials are equal:
\[
  \E{\R \sim \state(T-1)}{\pot(T-1,\R)} = \E{\R \sim \state(T)}{\pot(T,\R)}
\]
In Section~\ref{sec:potentials} we that this equation can be
used to create a potential function of all iterations, and how it can
be used to find the optimal strategies for both learner and adversary.

  Our ultimate result are min-max optimal bounds. However, there seem
  to be no matching min-max strategies for the original DTOL. To
  achieve min-max optimality we extend the game by enlarging the set of
  choices available to the adversary.  As the learner does not get
  additional choices, the min/max bound for the extended game is an
  upper bound on the average potential in the original game.

The rest of the paper is organizes as follows.

TBD

\section{related work}
Most of the papers on potential based online algorithms consider
one or a few potential functions. Most common is the exponential
potential, but others have been considered~\cite{cesa2006prediction}.
A natural question is what is the difference between potential
functions and whether some potential function is ``best''.

In this paper we consider a large set of potential functions,
specifically, potential functions that are strictly positive and have
strictly positive derivatives of orders up to four. The exponential
potential and the NormalHedge potential~\cite{chaudhuri2009parameter,luo2015achieving}
are member of this set. 

To analyze these potential functions we define a different
game, which we call the ``potential game''. In this game the primary
goal of the learner is not to minimize regret, rather, it is to
minimize the final score $\score^T$. To do so
we define potential functions for intermediate steps: $0 \leq t
<T$.\footnote{The analysis described here builds on a long line of
  work. Including the Binomial Weights algorithm and it's
  variants~\cite{cesa1996line,abernethy2006continuous,abernethy2008optimal}
  as well as drifting games~\cite{schapire2001drifting,freund2002drifting}.}

Zero-order bounds on the regret ~\cite{freund1999adaptive} depend only on $N$
and $T$ and typically have the form
\begin{equation} \label{eqn:0-order-bound}
  \max_j \R_j^T < C E \sqrt{T \ln N}
\end{equation}
for some small constant $C$ (typically smaller than 2).
These bounds can be extended to infinite sets of experts by defining
the $\epsilon$-regret of the algorithm as the regret with respect to
the best (smallest-loss) $\epsilon$-percentile of the set of experts.

this replaces the bound~(\ref{eqn:0-order-bound}) with 
\begin{equation} \label{eqn:0-epsilon-order-bound}
  \max_j \R_j^T < C E \sqrt{T \ln \frac{1}{\epsilon}}
\end{equation}

Lower bounds have been proven that match these upper bounds up to a
constant. These lower bounds typically rely on constructions in which
the losses $l_j^i$ are chosen independently at random to be either
$+1$ or $-1$ with equal probabilities.



Several algorithms with refined upper bounds on the regret have been
studied. Of those, the most relevant to our work is a paper by 
Cesa-Bianchi, Mansour and
Stoltz~\cite{cesa2007improved} on second-order regret bounds.
The bound given in Theorem~5 of ~\cite{cesa2007improved} can be
written, in our notation, as:
\begin{equation} \label{eqn:2nd-order-bound}
  \max_j \R_j^T \leq 4\sqrt{V_T \ln N} +2 \ln N +1/2 
\end{equation}
Where
\[
  \var_i = \sum_{j=1}^N P^i_j (l_j^i)^2 -  \left( \sum_{j=1}^N P^i_j
    l_j^i \right)^2 \mbox{ and } \V_T= \sum_{i=1}^T \var_i
\]

A few things are worth noting. First, as $|l_j^i|\leq 1$,
$\var_j\leq 1$ and therefor $V_T\leq T$. However $\V_T/T$ can be
arbitrarily small, in which case inequality~\ref{eqn:2nd-order-bound}
provides a tighter bound than ~\ref{eqn:0-order-bound}. Intuitively,
we can say that $\V_T$ replaces $T$ in the regret bound. This paper
provides additional support for replacing $T$ with $\V_T$ and provides
lower and upper bounds on the regret involving $\V_T$.

\section{Main Results}
\begin{enumerate}
\item {\bf Unifrm regret bound} There exists an online learning
  algorithm such that for any $\nu>0$ (set in advance) and any
  $t,\epsilon$ (holds uniformly) the following regret bound holds.
  \begin{equation}
\R_\epsilon \leq \sqrt{(t+\nu) \left( \ln (t+\nu) + 2 \ln \frac{1}{\epsilon}\right)}
\end{equation} 
\item {\bf Second order bound}
\item {\bf optimality of Brownian motion} For any potential function
  in $\SP{4}$ the min/max value of any state $(t,R)$ is attained by
  Brownian motion on the part of the adversary for any $s\geq t$.
\end{enumerate}


\section{Preliminaries} \label{sec:preliminaries}

We define some notation that will be used in the rest of the paper.

{\bf Positivity}
We require that potential functions have positive derivatives for a
range of degree. To that end we use the following definition:
\begin{definition}[Strict Positivity of degree $k$]
A function $f:\reals \to \reals$ is strictly positive of degree $k$, 
denoted $f \in \SP{k}$ if the derivatives of orders 0 to $k$:  
$f(x), \frac{d}{dx}f(x), \ldots, \frac{d^k}{dx^k}f(x)$ exist and are strictly positive.
\end{definition}
For the integer time game we assume that $\pot(\cdot) \in
\SP{2}$. Later on, in section~\ref{sec:smallsteps}, we will further
restrict our potential functions to be in $\SP{4}$.

{\bf Divisibility:} To reach optimality we need the set of actions to
be arbitrarily divisible. Intuitively, We replace the finite set of
actions with a continuous mass, so that each set of actions can be
partitioned into two parts of equal weight.  Formally, we define the
set of actions to be a probability space $(\Omega,\sigma,\mu)$ such
that $\omega \in \Omega$ is a particular action. We require that the
space is {\em arbitrarily divisible}, which means that for any
$s \in \sigma$ , there exist a partition $u,v \in \sigma$ such that
$u \cup v = s, u \cap v = \emptyset$, and
$\P{}{u}=\P{}{v}=\frac{1}{2} \P{}{s}$.

{\bf State:} The {\em state} of a game at iteration $i$, denoted $\state(i)$, is
a random variable that maps each action $\omega \in \Omega$ to the
cumulative regret of $\omega$ at time $i$: $\R_\omega^i$. The sequence
of cumulative regrets corresponding to action $\omega$ is the {\em
  path} of $\omega$:
\begin{equation} \label{eqn:path}
  S_{\omega}=(\R_\omega^1,\R_\omega^2,\ldots,\R_\omega^N)
\end{equation}

{\bf  Generalized binomial distribution}
We denote by $\Binom(n,s)$ the distribution over the reals defined by
$\sum_{i=1}^n X_i$ where $X_i$ are iid binary random variables which
attain the values $-s,+s$ with equal probabilities.

{\bf Expected value shorthand:} Suppose $P$ is a distribution over the reals, and $f:\reals
\to \reals$, we use the following short-hand notation for the expected
value of $f$ under the distribution $P$:
\[ P \odot f \doteq \E{x \sim P}{f(x)}  \]
We define the {\em score} at iteration $i$ as the average potential
with respect to the state:
\[ \score(i) = \state(i) \odot \pot(i) \doteq \E{\R \sim \state(i)}{\pot(i,\R)}\]
Note that in this short-hand notation we suppress the variable with
respect to which the integration is defined, which will always be $\R$.

{\bf Convolution:} Let $A,B$ be two independent random variables. We define the
convolution $A \oplus B$ to be the distribution of $x+y$. A constant
$a$ corresponds to the point mass distribution concentrated at
$a$. For convenience we define $A \ominus B = A \oplus (-B)$


\section{Integer time game}

The integer time game is described in
Figure~\ref{fig:integerTimeGame}.  The integer time game generalizes
the decision theoretic online learning problem~\cite{FreundSc97} in
the following ways:
\begin{enumerate}
\item The goal of the learner in DTOL is to guarantee an upper bounds
  on the regret. The learner's goal in the integer time game is to
  minimize the final score. From theorem~\ref{thm:simulBoundAveragePot} we know that if
  we set the final potential as $\finalPotT(\R) = \frac{1}{G(\R)}$ then the two
  conditions are equivalent, allowing us to focus on the score.
\item The number of iterations $T$ is given as input, as is the
  potential function at the end: $\finalPotT(\R)$.
\item The actions are assumed to be {\em divisible}. For our purposes
it is enough to assume that any action can be split into two equal
weight parts.
\end{enumerate}

The key to the potential based analysis is that usiing the predefined
final potential we can define potential functions and scores for all
iterations $1,\ldots,T-1$. This is explained in the next subsection.

\begin{figure}[ht]
\framebox{
\begin{minipage}[t]{6.4in}
Initialization:
\begin{itemize}
\item input: $T$ : The number of iterations.
\item Final iteration potential function:  $\finalPotT \in \SP{2}$
\item $\state(1) = \delta(0)$ is the initial state of the game which
  is a point mass distribution at 0. 
\end{itemize}

For $i=1,2,\ldots,T$:\\

\begin{enumerate}
\item The learner chooses a non-negative random variable over $\Omega$
  that is the {\em weight function} $\learnerM(i,\R)$ such that
  $\state(i) \odot \learnerM(i)=1$
\item The adversary chooses a function $\adversM(i,\R)$ that maps
  $i,\R$ to a distribution over $[-1,+1]$. This
  random variable corresponds to the instantanuous loss of each action at
  time $t$.
\item 
  We define the {\em bias} at $(i,\R)$ to be
  \begin{equation} \label{eqn:Bias}
    \Bias(i,\R) \doteq \E{l \sim \adversM(i,\R)}{l}
  \end{equation}
\item the average loss is 
  \begin{equation} \label{eqn:aggregate-loss}
    \ell(i)=\state(i,\R) \odot \paren{\learnerM(i,\R) \Bias(i,\R)}
  \end{equation}
\item The state is updated. 
  \begin{equation} \label{eqn:state-update}
    \state(i+1) = \E{\R \sim \state(i)}{\R \oplus  \adversM(i,\R)}
    \oplus -\ell(i)
    \end{equation}
  Where $\adversM(i,\R)$ is the distribution of the losses of experts
  with respect to which the regret is $\R$ after iteration
  $i-1$. $\oplus$ denotes the convolution as defined above.
\end{enumerate}

The final score is calculated: $\score(T)=\state(T) \odot
\finalPotT$.

The goal of the learner is to minimize this score, the goal
of the adversary is to maximize it.
\end{minipage}}
\caption{The integer time game \label{fig:integerTimeGame}}
\end{figure}

\subsection{Defining potential Functions for all iterations\label{sec:potentials}}

The potential game defines the {\em final} potential function
$\finalPotT$, at the end of the game. We will now show, that we can
extend the definition of a potential function to all iterations of the game.

 A single action defines a path $S_\omega$ (as defined in (\ref{eqn:path})). Fixing
 the strategies of the learner and the adversary determines
 a distribution $\D$ over paths.
We describe two equivalent ways to define $\potPQ(i,\R)$ for $i<T$
 \begin{enumerate}
 \item{\bf Using conditional expectation} We can define the potential
   on iteration $i$ based on the fixed potential at iteration $T$.
Using the definition of prefix sum given
in Equation~(\ref{eqn:prefix-sum}, we can write this conditional
expectation as 
\begin{equation}
  \forall i=1,\ldots,T, \R \;\;\;
  \potPQ(i,\R)=\E{\omega\sim \D|\R_\omega^i=\R}{\pot(T,\R_{\omega}^T)}
\end{equation}
 \item{\bf Using backward induction} It is sometimes convenient to
   compute the the potential for time $i$ from the potential at time
   $i+1$:
   \begin{equation} \label{eqn:back-induction}
     \forall i=1,\ldots,T-1, \R\;\;\;
     \potPQ(i,\R)=\E{\omega\sim \D|M_\omega^i=\R}{\potPQ(i+1,\R_{\omega}^{i+1})}
   \end{equation}
by using backwards induction: $i=T-1,T-2,\ldots,1$ we can compute the
potential for all iterations.


We use Equations~(\ref{eqn:Bias},\ref{eqn:aggregate-loss}) and
marginalizing over $\R$ to express Equation~(\ref{eqn:back-induction})
in terms of the single step strategies:
\begin{equation} \label{eqn:induction}
  \forall i=1,\ldots,T-1, \R\;\;\;
  \potPQ(i,\R) \doteq \E{r \sim [(\R-\ell(i)) \oplus \adversM(i,\R)]}{\potPQ(i+1,r)}
\end{equation}

\end{enumerate}

The score at iteration $i$ is defined as
$\score(i)=\state(i)\odot \pot(i)$. The scores are all different
expressions for calculating the expected final potential for the fixed strategies
$\adversM, \learnerM$. Therefor the scores are all equal, as expressed
in the following theorem:

\begin{theorem} \label{thm:backward-recursion}
Assuming $ \learnerM(i,\R), \adversM(i,\R)$ are fixed for all
$i=1,\ldots,T-1$, then
\[
  \state(T)\odot \pot(T)=\score(T)=\score(T-1)=\cdots=\score(1)=\potPQ(0,0)
  \]
\end{theorem}

A few things worth noting:
\begin{enumerate}
\item $\potPQ(i,\R)$ is the the final expected potential
  given that the paths starts at $(i,\R)$ and that
  the strategies used by both players in iterations $i,\ldots,T$ are fixed. Note
  also that which strategies were used in iterations $1,\ldots,i-1$ is
  of no consequence. The effect of past choices is captured by the
  state $\state(i)$.
\item
  The final expected potential is equal to $\pot(0,0)$ which is the
  potential at the common starting point: $i=1$, $\R=0$.
\end{enumerate}

\subsection{Upper and Lower potentials}
Next,we vary the strategies of one side or the other to define upper
and lower potentials.
\begin{equation} \label{eqn:upperPotentials}
  \exists \learnerM, \;\;\; \forall \adversM,\;\; \forall 1\leq i \leq
  T,\;\; \forall \R \in \reals,\;\; \upperpot(i,\R) \geq \potPQ(i,R)
\end{equation}
\begin{equation} \label{eqn:lowerPotentials}
  \exists \adversM, \;\;\; \forall \learnerM, \;\; \;\; \forall 1\leq i \leq
  T,\;\; \forall \R \in \reals,\;\; \lowerpot(i,\R) \leq \potPQ(i,R)
\end{equation}

In words, $\upperpot$ is an upper bound on the potential that is 
guaranteed by the learner strategy $\learnerM$ while $\lowerpotd$
is a lower bound that is guaranteed by the adversarial
strategy $\adversM$.

Following the same argument as the one leading to
Theorem~\ref{thm:backward-recursion}. We define upper and lower scores
$\upperscoreM(i),\lowerscoreM(i)$ such that
\begin{equation} \label{eqn:upper-recursion}
  \state_{\learnerM}(T)\odot \finalPotT=\upperscoreM(T)=\upperscoreM(T-1)=\cdots=\upperscoreM(0)=\upperpot(0,0)
\end{equation}
and
\begin{equation} \label{eqn:lower-recursion}
  \state_{\adversM}(T)\odot \finalPotT=\lowerscoreM(T)=\lowerscoreM(T-1)=\cdots=\lowerscoreM(0)=\lowerpot(0,0)
\end{equation}

Our ultimate goal is to find strategies $\learnerM$ and
$\adversM$ such that
\begin{equation} \label{eqn:limitPotential}
\forall i,\R,\;\;\; \lowerpot(i,\R) = \upperpot(i,\R)
\end{equation}
in particular, $\lowerscoreM(0)=\lowerpot(0,0) =
\upperpot(0,0)=\upperscoreM(0)$. This means that
$\adversM,\learnerM$ are a min/max pair of strategies and that
$\lowerscoreM(0)=\upperscoreM(0)$ define the min/max value of the game.
~\\~\\
We do not achieve this for the integer game described in the next
section. To achieve min/max optimality we extend the integer time game
to the discrete time game (section~\ref{sec:discrete}) and to the
continuous time game (\ref{sec:continuous}).

\subsection{Strategies for the integer time  game} \label{sec:strat-integer}
We assume that $\finalPotT  \in \SP{2}$, in other words, the final
potential is positive, increasing and convex. $\finalPotT$ defines the
upper and lower potentials at time $T$:
$$\lowerpotb(T,\R) = \upperpotb(T,\R) = \finalPotT(\R) $$
We define a backwards recursion for the lower potential:
\begin{equation} \label{eqn:backward-iteration-lower}
  \lowerpotb(i-1, \R) = \frac{\lowerpotb(i,\R+1) + \lowerpotb(i,\R-1)}{2}
\end{equation}
and a backwards recursion for the upper potential:
\begin{equation} \label{eqn:backward-iteration-upper-recursion}
  \upperpotb(i-1, \R) = \frac{\upperpotb(i,\R+2) + \upperpotb(i,\R-2)}{2}
\end{equation}

We define strategies that correspond to these potentials. A strategy
for the adversay:
\begin{equation} \label{eqn:adv-strat-p}
  \adversMb(i,\R) =
  \begin{cases}
    +1 & \mbox{ w.p. } \frac{1}{2}\\
    -1 & \mbox{ w.p. } \frac{1}{2}\\
  \end{cases}
\end{equation}
and a strategy for the learner:
\begin{equation} \label{eqn:learner-strat-1}
\learnerMb(i,\R) = \frac{1}{Z} \frac{\pot(i,\R+2) - \pot(i,\R-2)}{2}
\end{equation}
Where $Z$ is a normalization factor
$$Z = \E{\R \sim \state(i)}{\frac{\pot(i,\R+2) - \pot(i,\R-2)}{2}}$$


The following lemma states that these strategies guarantee the
corresponding potentials.
\begin{lemma} \label{lemma:first-order-bound}
~\\
Let $i$ be an integer between $1$ and $T$

If $\lowerpotb(i,\R) \in \SP{2}$
\begin{enumerate}
\item  $\lowerpotb(i-1,\R) \in \SP{2}$
\item The adversarial strategy~(\ref{eqn:adv-strat-p})
  guarantees the recursion given in Eqn.~(\ref{eqn:backward-iteration-lower})
\end{enumerate}

If $\upperpotb(i,\R) \in \SP{2}$
\begin{enumerate}
\item $\upperpotb(i-1,\R) \in \SP{2}$
\item The learner strategy~(\ref{eqn:learner-strat-1})
  guarantees the recursion given in Eqn.~(\ref{eqn:backward-iteration-upper-recursion})
\end{enumerate}

\end{lemma}

\proof~\\
\begin{enumerate}
  \item{\bf Convexity:} If $\lowerpotb(i-1,\R)$ is a convex combination
    of $\lowerpotb(i,\R)$. Therefor $\lowerpotb(i,\R) \in \SP{2}$
    implies $\lowerpotb(i-1,\R) \in \SP{2}$. A similar argument holds
    for $\upperpotb$.
\item{\bf Adversary:} By symmetry adversarial strategy~(\ref{eqn:adv-strat-p}) guarantees that
  the aggregate loss~(\ref{eqn:aggregate-loss}) is zero regardless of
  the choice of the learner: $\ell(i)=0$.
  Therefor the state update~(\ref{eqn:state-update}) is equivalent to
  the symmetric random walk:
  $$\state(i) = \frac{1}{2} \paren{(\state(i) \oplus 1) + (\state(i)
    \ominus 1)}$$
  Which in turn implies that if the adversary plays $\adversM^*$
  and the learner plays an arbitrary strategy $\learnerM$
  \begin{equation} \label{eqn:lower}
    \lowerpotb(i-1,\R) = \frac{\lowerpotb(i,\R-1)+\lowerpotb(i,\R+1)}{2}
  \end{equation}
  As this adversarial strategy is oblivious to the learner's strategy, it
  guarantees that the average value at iteration $i$ is {\em equal} to the
  average of the lower value at iteration $i$.
\item {\bf Learner:}
  Plugging learner's strategy~(\ref{eqn:learner-strat-1})
  into equation~(\ref{eqn:aggregate-loss}) we find that
 \begin{equation} \label{eqn:ell-optimal-learner}
   \ell(i) = \frac{1}{Z_{i}} \E{\R \sim \state(i)}{\paren{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}
   \Bias(i,\R)}
\end{equation}
  Consider the score at iteration $i$ when the learner's strategy
  is $\learnerM^*$ and the adversarial strategy  $\adversM$ is arbitrary
     \begin{equation} \label{eqn:Pot-Update}
    \score_{\learnerM^*,\adversM}(i,\R) = \E{\R \sim \state(i)}{ \E{y \sim
      \adversM(i)(\R)}{\pot(i,\R+y-\ell(i))}}
  \end{equation}
  As $\pot(i,\cdot)$ is convex and as $y-\ell(i) \in [-2,2]$,
  \begin{equation} \label{eqn:pot-upper}
    \upperpotb(i-1,\R+y) \leq \frac{\upperpotb(i,\R+2)+\upperpotb(i,\R-2)}{2} +
    (y-\ell(i)) \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}
    \end{equation}
  Combining the equations~(\ref{eqn:ell-optimal-learner}) and~(\ref{eqn:Pot-Update}) we find that
  \begin{eqnarray}
  \score_{\learnerM^*,\adversM}(i,\R)&=&\E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{\upperpotb(i,\R+y-\ell(i))}}\\
  &\leq & \E{\R \sim \state(i)}{\frac{\upperpotb(i,\R+2)+\upperpotb(i,\R-2)}{2}}\\
  &+&
  \E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{(y-\ell(i)) \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}} \label{eqn:zero-term}
  \end{eqnarray}
  
The final step is to show that the term~(\ref{eqn:zero-term}) is equal
to zero. As $\ell(i)$ is a constant with respect to $\R$ and $y$ the
term~(\ref{eqn:zero-term}) can be written as:
\begin{eqnarray}
&&\E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{(y-\ell(i))
   \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}}\\
&=&
\E{\R \sim \state(i)}{\Bias(i,\R)
    \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}\\
  &-& \ell(i) \E{\R \sim \state(i)}{
    \frac{\upperpotb(i,\R+2)-\upperpotb(i,\R-2)}{2}}\\
  &=& 0
\end{eqnarray}
\end{enumerate}
\qed

Repeating the induction steps of Lemma~\ref{lemma:first-order-bound}
from $i=T$ to $i=1$ yields the following theorem.
\begin{theorem} \label{thm:IntegerGameBounds}
  Let $\finalPotT \in \SP{2}$, for any iteration $0 \leq i \leq T$ and
  regret $\R_0 \in \reals$ 
  \begin{itemize}
  \item
    The lower potential gearenteed by $\adversMb$ is
     $$\lowerpotb(i,R_0) = \E{\R \sim \R_0 \oplus \Binom(T-i,1)}{\finalPotT(\R)} $$
  \item
    The upper potential guaranteed by $\learnerMb$ is
    $$\upperpotb(i,R_0) = \E{\R \sim \R_0 \oplus \Binom(T-i,2)}{\finalPotT(\R)}$$
  \end{itemize}
\end{theorem}

Plugging in $i=0$,$\R=0$ we ge the following corrolary:
\begin{corollary}
  if the learner plays $\learnerMb$ on every iteration
  it guarantees that the final score satisfies
  $$\state(T) \odot \finalPotT \leq \Binom(T,2) \odot \finalPotT $$

  If the Adversary plays $\adversMb$ on every iteration it guarantees
  that:
  $$\state(T) \odot \finalPotT = \Binom(T,1) \odot \finalPotT $$
\end{corollary}
\section{From integer to discrete time}
\label{sec:discrete}

The upper and lower bound on the final score given in
Theorem~\ref{thm:IntegerGameBounds} do not match. If
$\finalPotT \in \SP{2}$ then \linebreak
$\Binom(T,1) \odot \finalPotT <\Binom(T,2) \odot \finalPotT$ In other
words, the
strategies~(\ref{eqn:adv-strat-p},\ref{eqn:learner-strat-1}) are not a
min/max pair.\footnote{There might be other (pure) strategies for the
  integer game that are a min/max pair, we conjecture that is not the
  case, and seek a extension of the game that would yield min/max
  strategies.}

To close this gap we extend the integer time game into a new game we
call the discrete time game (Fig.~\ref{fig:discrete-Time-Game}). The
discrete time game increases the options available to the adversary,
but not to the learner.  As the integer step game is a special case of
the new game, any upper potential that can be guaranteed by the
learner in the discrete time game is also an upper potnetial for the
discrete time game.

In the integer time game the loss of each action is in the range
$[-1,+1]$, in the discrete time game the adversary chooses, on
iteration $i$ a step size $0<s_i\leq 1$ which restricts the losses to
the range $[-s_i,+s_i]$. Note that by always choosing $s_i=1$,
the adversary can choose to play the integer time game.

\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
Initialization: $t_0=0$ \newline
  
On iteration $i=1,2,\ldots$
\begin{enumerate}
\item  If $t_{i}= \realT$ the game terminates.
\item The adversay chooses a {\em step size} $0<s_i\leq \min(\sqrt{1-t_i},1)$, which advances
  time by $t_i = t_{i-1}+s_i^2$
\item Given $s_i$, the learner chooses a distribution $\learnerM(i)$ over $\reals$.
\item The adversary chooses a mapping from $\reals$ to distributions
  over $[-s_i,+s_i]:\;\adversM(t,\cdot): \reals \to \Delta^{[-s_i,+s_i]}$
  
\item The aggregate loss is calculated:
  \begin{equation} \label{eqn:ell-discrete}
    \ell(t_i)=\E{\R \sim \state(t_i)}{\learnerM(t_i,\R) \Bias(t_i,\R)}
    \mbox{ where } \Bias(t_i,\R) \doteq \E{y \sim \adversM(t_i,\R)}{y}
  \end{equation}
  Such that $|\ell(t_i)| \leq s_i^2$
\item The state is updated. 
  $$\state(t_i) = \E{\R \sim \state(t_{i})}{\adversM(t_i,\R) \oplus (\R-\ell(t_i))}
  $$
  Where $\oplus$ is a convolution as defined in the preliminaries.
\end{enumerate}

Upon termination, the final value is calculated:
$$\score(\realT) =\state(\realT) \odot \pot(\realT)$$

\end{minipage}}
\caption{The discrete time game  \label{fig:discrete-Time-Game}}
\end{figure}

We make two additional alterations to the integer time game in order to
keep the game fair. An unfair game is one where one side always wins.
We list the alterations and then justify them.
\begin{enumerate}
\item {\bf real-valued time} In the integer time game we use an
  integer to indicate the iteration number: $i=1,2,\ldots,T$. In the
  discrete time game we use an positive real value, which we call
  ``time'' and use the update rule $t_{i+1} = t_i + s_i^2$, and define
  the final time, which is used in the regret bound, to be
  $\realT=\sum_{i=0}^T s_i^2$
\item {\bf Bounded average loss} We restrict the average loss to a
  range much smaller than $[-s_i,+s_i]$, specifically:
  $|\ell(i)| \leq s_i^2$
\end{enumerate}
Note that both of these conditions hold trivially when $s_i=1$
\begin{enumerate}
  \item {\bf Justification of real-valued time}
To justify these choices we consider the following adversarial
strategy for the discrete time game:
\begin{equation} \label{eqn:adv-strat-s}
  \adversMd \brac{s,p} \paren{t,R} =
  \begin{cases}
    +s & \mbox{ w.p. } p\\
    -s & \mbox{ w.p. } 1-p\\
  \end{cases}
\end{equation}

From Equation~(\ref{eqn:lower-recursion}) we get that the initial
score,
\[
  \lowerscoreMd(0) = \lowerscoreMd(T)=\state_{\adversMd}(T) \odot \pot(T)
\]

On the other hand, we know that  $\state_{\adversMd}(T) =
\Binom(T,s)$. Suppose $T$ is large enough that the normal approximation for the
binomial can be used. Let ${\cal N}(\mu,\sigma^2)$ be the normal
distribution with mean $\mu$ and variance $\sigma^2$.
\begin{equation}   \label{eqn:largeTlimit}
  \lim_{T \to \infty} \lowerscoreMd(0) =  {\cal N}(0,Ts^2) \odot \pot(T)
\end{equation}

Recall that $\pot(T)$ is a fixed strictly convex function. It is not
hard to see that if $Ts^2 \to 0$ minimizes  $\lowerscoreMd(0)$ and
makes it equal to to $\pot(T,0)$, which means that the learner wins,
while if $Ts^2 \to \infty$, $\lowerscoreMd(0) \to \infty$ which means
that the adversary wins. In order to keep the game balanced
keep $Ts^2$ constant as we let $s \to 0$. We achieve that by defining
the real-valued discrete time as $t_j = \sum_{i=0}^{j-1} s_i^2$.

\item {\bf Justification of bounding average loss} Suppose the game is
  played for $T$ itertions and that the adversary uses the strategy
  $\adversMd\brac{s,\frac{1}{2}+\epsilon}\paren{t,\R}$ and that
  $s=\frac{1}{\sqrt{T}}$. In this case the loss of the learner in
  iteration $i$ is $\ell(i)=2s\epsilon$ and the total loss is
    $$L_\ell^T=\sum_{i=0}^{T-1} \ell(i) = T 2 \epsilon s = \frac{2 \epsilon}{s}$$.

    If $\epsilon$ is kept constant as $s \to 0$
    then $\lim_{T \to \infty}L_\ell^T=\infty$, biasing the game towards the adversary. On the other
    hand, if $\epsilon =s^{\alpha}$ for $\alpha<1$ then $L_\ell^T
    \to 0$, biasing the game towards the learner. To keep the game
    balanced we have to set $\epsilon=cs$ for some constant
    $c$. Withoutloss of generality we set $c=1$.

    Generaliziing this to the game where the adversary can choose a
    different $s_i$ in each iteration we get the constraint 
    $|\ell(i)| \leq s_i^2$
\end{enumerate}

\subsection{Strategies for discrete time game}
We fix an integer number $\realT$ as the ``real'' length of the
game.

We define a sequence of adversarial strategies $\adversMdk$ for
$k=1,2,\ldots$. The $k$'th strategy uses step size $s_k=2^{-k}$ and
time increments $\Delta t_k=s_k^2=2^{-2k}$.
We set the time points $t_i =i s_k^2$ for $i=0,1,\ldots,T-1$ where $T=\realT/s_k^2$

For a given $k$ we define upper and lower potentials for each
$t_i$. This is done by induction starting the fixed final potential
function  $\finalPotT(\R)=\lowerpotMdk(\realT, \R)=\upperpotMdk(\realT, \R)$ and iterating
backwards for $t_i = i \Delta t$,  $i=T,T-1,\ldots,0$
 \begin{equation} \label{eqn:backward-iteration-lower-discrete}
   \lowerpotMdk(t_{i-1}, \R) = \frac{\lowerpotMdk(t_i,\R+s_k) + \lowerpotMdk(t_i,\R-s_k)}{2}
 \end{equation}


 \begin{equation} \label{eqn:backward-iteration-upper-recursion-discrete}
   \upperpotMdk(t_{i-1}, \R) = \frac{\upperpotMdk(t_i,\R+s_k(1+s_k)) + \upperpotMdk(t_i,\R-s_k(1+s_k))}{2}
 \end{equation}

These upper and lower potentials correspond to strategies for the
adversary and the learner.
The adversarial strategy is 
\begin{equation} \label{eqn:adv-strat-dk}
  \adversMdk=  \begin{cases}
    +s_k & \mbox{ w.p. } \frac{1}{2}\\
    -s_k & \mbox{ w.p. } \frac{1}{2}\\
  \end{cases}
\end{equation}

The learner's strategy is:
\begin{eqnarray} \label{eqn:learner-strat-1c}
  \learnerMdk(t_{i},\R) = \frac{1}{Z}
  \frac{\upperpotMdk(t_{i+1},\R+s_k(1+s_k)) -
  \upperpotMdk(t_{i+1},\R-s_k(1+s_k))}{2} \\
  \mbox{ where } Z = \E{\R \sim \state(t_{i+1})}{\frac{\upperpotMdk(t_{i+1},\R+s_k(1+s_k)) -
  \upperpotMdk(t_{i+1},\R-s_k(1+s_k)}{2}} \nonumber
\end{eqnarray}

The potentials and strategies defined above are scaled versions of the
integer time potential recursions defined in
Equations~(\ref{eqn:backward-iteration-lower},\ref{eqn:backward-iteration-upper-recursion})
and the strategies defined in
Equations~(\ref{eqn:adv-strat-p},\ref{eqn:learner-strat-1}).  They are
identical when $s=1$.

The following Lemma parallels Lemma~\ref{lemma:first-order-bound}
\begin{lemma} \label{lemma:discrete-step-bound}
~\\
Let $i$ be an integer between $1$ and $T$

If $\lowerpotMdk(t_i,\R) \in \SP{2}$
\begin{enumerate}
\item  $\lowerpotMdk(t_{i-1},\R) \in \SP{2}$
\item The adversarial strategy~(\ref{eqn:adv-strat-dk})
  guarantees the recursion given in Eqn.~(\ref{eqn:backward-iteration-lower-discrete})
\end{enumerate}

If $\upperpotMdk(t_i,\R) \in \SP{2}$
\begin{enumerate}
\item $\upperpotMdk(t_{i-1},\R) \in \SP{2}$
\item The learner strategy~(\ref{eqn:learner-strat-1c})
  guarantees the recursion given in Eqn.~(\ref{eqn:backward-iteration-upper-recursion-discrete})
\end{enumerate}

\end{lemma}
\proof
The statement of the Lemma and the proof are scaled versions of
Lemma~\ref{lemma:first-order-bound} and its proof. The iteration step
is $s_k^2$ instead of $1$ while the loss/gain of an expert in a single
step is $[-s_k,s_k]$ instead of $[-1,+1]$.

One change worth noting is at the step from
Equation~(\ref{eqn:Pot-Update}) and Equation~(\ref{eqn:pot-upper}),
where the bound  $y-\ell(i) \in [-2,2]$ is replace by  $y-\ell(i) \in
[-s_k-s_k^2,s_k+s_k^2]$. This follows from the bound $|\ell(i)| \leq
s_k^2$ which is discussed in Section~\ref{sec:discrete}.
\qed

\begin{theorem} \label{thm:IntegerGameBounds}
  Let $\finalPotR \in \SP{2}$ be the final potential in the discrete
  time game. Fix the step size $s_k=2^{-k}$, let
  $\realT$ be an integer and define 
  $T=\frac{\realT}{s_k^2}$. Let $t_i=i s_k^2$ for $i=0,1,\ldots,T$ and
  let $R_0$ be some regret value, then

  \begin{itemize}
  \item
    The lower potential gearenteed by $\adversMdk$ is
     $$\lowerpotMdk(t_i,R_0) = \E{\R \sim \R_0 \oplus \Binom(\realT-t_i,s_k)}{\finalPotT(\R)} $$
  \item
    The upper potential guaranteed by $\learnerMdk$ is
    $$\upperpotMdk(t_ii,R_0) =  \E{\R \sim \R_0 \oplus \Binom(\realT-t_i,s_k(1+s_k))}{\finalPotT(\R)} $$
  \end{itemize}

\end{theorem}


{\bf ToDo:}  Go back to the large T limit in \ref{eqn:largeTlimit} and
show that in that limit the upper and lower potentials converge to
each other.  It remains to show that the adversary will prefer smaller steps.


\begin{theorem} \label{thm:seq-of-adv-strategies}
  ~\\
  Assume $\pot(\realT,\R) \in \SP{4}$ and 
   let $A = N(0,\sqrt{\realT}) \odot \pot(\realT)$. then
   for any $\epsilon>0$
   \begin{itemize}
     \item
    There exists a strategy for the adversary that guarantees, against
    any learner, that $\score(T) \geq A-\epsilon$
  \item
    There exists a strategy for the learner that guarantees, against
    any adversary, that $\score(T) \leq A+\epsilon$.
  \end{itemize}
\end{theorem}


\subsection{The adversary prefers smaller steps} \label{sec:smallsteps}
As noted before, if the adversary chooses $s_i=1$ for all $i$ the game
reduces the the integer time game. The question is whether the
adversary would prefer to stick with $s_i=1$ or instead prefer to use
$s_i<1$. In this section we give a somewhat surprising answer to this question
-- the adversary {\em always} prefers a smaller value of $s_i$ to a larger
one. This leads to a preference for $s_i \to 0$, as it turns out, this
limit is well defined and corrsponds to Brownian motion, also known as
Wiener process.

Consider a sequence of adversarial strategies $S_k$ indexed by
$k=0,1,2,$. The adversarial strategy $S_k$ is corresponds to always
choosing $s_i = 2^{-k}$, and repeating  $\adversM^{1/2}_{\pm 2^{-k}}$ 
for $T 2^{2k}$ iterations.
This corresponds to the distribution created by a random walk with
$T 2^{2k}$ time steps, each step equal to $+2^{-k}$ or  $-2^{-k}$ with probabilities $1/2,1/2$.
Note that in order to preserve the variance, halving the step size
requires incresing the number of iterations by a factor of four.

Let $\pot(S_k,t,\R)$ be the value associated with adversarial
strategy $S_k$, time $t$ (divisible by $2^{-2k}$) and
location $\R$. We are ready to state our main theorem.

\begin{theorem}\label{thm:smallerSteps}
  If the final value function has a strictly positive fourth
  derivative:
  $$ \frac{d^4}{d \R^4} \finalPotR(\R) >0, \forall \R$$
  then for any integer $k>0$ and any $0 \leq  t \leq T$, such that $t$
  is divisible by
  $2^{-2k}$ and any $\R$,
  $$\pot(S_{k+1},t,\R)) >  \pot(S_{k},t,\R)$$
\end{theorem}

Before proving the theorem, we describe it's
consequence for the online learning problem.
We can restrict Theorem~\ref{thm:smallerSteps} for the
case $t=0$,$\R=0$ in which case we get an increasing sequence:
\[
\pot(S_1,0,0) < \pot(S_2,0,0) <\cdots <\pot(S_k,0,0) <
\]
The limit of the strategies $S_k$ as $k \to \infty$ is the well
studied Brownian or Wiener process. We will discuss this connection in Section~\ref{}.

We now go back to proving Theorem~\ref{thm:smallerSteps}. The core of
the proof is a lemma which compares, essentially, the value recursion
when taking one step of size 1 to four steps of size 1/2.


Consider the advesarial strategies $S_k$ and $S_{k+1}$ at a particular
time point $0 \leq t \leq T$ such that $t$ is divisible by
$\deltat=2^{-2k}$ and at a particular location $\R$. Let
$t'=t+\deltat$, and fix a value
function for time , $\pot(t',\R)$ and compare between
two values at $\R,t$. The first value denoted
$\pot_k(t,\R)$ corresponds to $S_k$, and consists of a single random step of $\pm 2^{-k}$. 
The other value $\pot_{k+1}(t,\R)$ corresponds to $S_{k+1}$ and consists of
four random steps of size $\pm 1/2$.

\begin{lemma} \label{lemma:n-strictly-convex}
If $\pot(t',\R)$ as function of $\R$ is in $\SP{4}$, then 
\begin{itemize}
\item $\pot_k(t,\R) < \pot_{k+1}(t,\R)$
\item Both $\pot_k(t,\R)$ and $\pot_{k+1}(t,\R)$ are in $\SP{4}$.
\end{itemize}
\end{lemma}

\proof
Recall the notations $\deltat = 2^{-2k}$ $t' = t+\deltat$ and $s=2^{-k}$.
We can write out explicit expressions for the two values:
\begin{itemize}
\item For strategy $S_0$ the value is
  $$\pot_k(t, \R) = \frac{\pot(t',\R+s)+ \pot(t',\R-s)}{2} $$.
\item For strategy $S_1$ the value is
  $$\pot_{k+1}(t, \R) = \frac{1}{16}
  \paren{\pot(t',\R+2s)+ 4\pot(t',\R+s)+ 6\pot(t',\R)+  4\pot(t',\R-s)+ \pot(t',\R-2s)}
  $$.
\item the difference between the values is
 $$ \pot_{k+1}(t, \R)-\pot_{k}(t, \R) = \frac{1}{16}
  \paren{\pot(t',\R+2s)- 4\pot(t',\R+s)+ 6\pot(t',\R)-
    4\pot(t',\R-s)+ \pot(t',\R-2s)}
  $$
\item Our goal is to show that the RHS is positive. Therefor we can
  divide it by the positive constant  $\frac{2}{3} s^4$, and call the
  resulting function $f$:
  \begin{equation} \label{eqn:divdiff}
  g(\R)=\frac{1}{24s^4} \paren{\pot(t,\R+2s)- 4\pot(t,\R+s)+ 6\pot(t,\R)-
    4\pot(t,\R-s)+ \pot(t,\R-2s)}
  \end{equation}
\end{itemize}

The function $g(\R)$ has a special form called ``divided difference''
that has been extensively studied ~\cite{popoviciu1965certaines,butt2016generalization, de2005divided}.
and is closely related to to derivatives of different orders. Using
this connection and the fact that $\pot(\cdot,\R) \in \SP{4}$ we prove
the following lemma:

The following lemma states that $g(\R)>0$ for all $\R$.
\begin{lemma} \label{lemma:divdiff}
Fix $t>0$ and $s>0$, let $\pot(t,\R) \in \SP{4}$ as a function of $\R$,
and let 
\begin{equation}
g(\R)\doteq\frac{1}{24s^2} \paren{\pot(t,\R+2s)- 4\pot(t,\R+s)+ 6\pot(t,\R)-
    4\pot(t,\R-s)+ \pot(t,\R-2s)}
\end{equation}
then $\forall \R,\; g(\R)>0$ 
\end{lemma}
The proof is given in appendix~\ref{sec:divdiff}

We conclude that if $\pot(t',\R)$ has a strictly positive fourth
derivative then $\pot_{k+1}(t,\R) > \pot_{k}(t,\R)$ for all $\R$, proving
the first part of the lemma.

The second part of the lemma follows from the fact that
both $\pot_{k+1}(t,\R)$ and $\pot_{k}(t,\R)$ are convex combinations of
$\pot(t,\R)$ and therefor retain their continuity and convexity properties.

\qed

\proof  of Theorem~\ref{thm:smallerSteps} \\
The proof is by double induction over $k$ and over $t$.
For a fixed $k$ we take a finite backward induction over
$t=T-2^{-2k},T-2 \times 2^{-2k},T-3 \times 2^{-2k},\cdots,0$.
Our inductive claims are that $\pot_{k+1}(t,\R) > \pot_{k}(t,\R)$ and
$\pot_{k+1}(t,\R)$,$\pot_{k}(t,\R)$ are continuous, strongly convex and
have a strongly positive fourth derivative. That these claims carry over
from $t=T-i \times 2^{-2k}$ to  $t=T-(i+1) \times 2^{-2k}$ follows
directly from Lemma~\ref{lemma:n-strictly-convex}.

The theorem follows by forward induction on $k$.

\qed


\iffalse
Next, we consider the discrete time version of $\learnerM^2$:
(Eqn~\ref{eqn:learner-strat-2})
\begin{eqnarray} \label{eqn:learner-strat-2c}
  \learnerM^{2d}(t_{i},\R) =  \frac{1}{Z^{2d}}
  \left. \frac{\partial}{\partial r} \right|_{r=\R}
  \pot(t_{i}+s_{i}^2,r)
  \\
  \mbox{ where } Z^{2d} = \E{\R \sim
  \state(t_i)}{\left. \frac{\partial}{\partial r} \right|_{r=\R} \pot(t_{i}+s_{i}^2,r)} \nonumber
\end{eqnarray}
\fi

\section{Brownian motion and min/max strategies}
\label{sec:continuous}

In the previous section we described a sequence of adversarial
strategies $S_1,S_2,\ldots$ and a learner strategy such that ....

described a sequence of adversarial
strategies

It is well known that the limit of random walks where $s \to 0$ and
$\deltat=s^2$ is the the Brownian or Wiener process
(see~\cite{kac1947random}).

An alternative characterization of Brownian Process is
$$ \P{}{X_{t+\deltat}=x_1 | X_t=x_0}=e^{-\frac{(x_1-x_0)^2}{2 \deltat}}$$

The backwards recursion that
defines the value function is the celebrated Backwrds Kolmogorov
Equation with no drift and unit variance
\begin{equation} \label{eqn:Kolmogorov}
  \frac{\partial}{\partial t} \pot(t,\R)
  + \frac{1}{2} \frac{\partial^2}{\partial \R^2} \pot(t,\R)=0
\end{equation}
Given a final value function with a strictly positive fourth
derivative we can use Equation~(\ref{eqn:Kolmogorov}) to compute the
value function for all $0 \leq t \leq T$. We will do so in he next section.

\section{Stable potential functions and anytime strategies} \label{sec:stable}

The potential functions, $\pot(t,\R)$ is a solution of PDE~(\ref
{eqn:Kolmogorov}):
\begin{equation} 
  \frac{\partial}{\partial t} \pot(t,\R)
  + \frac{1}{2} \frac{\partial^2}{\partial r^2} \pot(t,\R)=0
\end{equation}
under a boundary condition $\pot(T,\R)=\finalPotR(\R)$, which we assume
is in $\SP{4}$

So far, we assumed that the game horizon $T$ is known in advance. We
now show two value functions where knowledge of the horizon is not
required. Specifically, we call a value function $\pot(t,\R)$
{\em self consistent} if it is defined for all $t>0$ and if for any
$0<t<T$, setting $\phi(T,\R)$ as the final potential and solving for
the Kolmogorov Backward Equation yields $\phi(t,\R)$ regarless of the
time horizon $T$. 

We consider two solutions to the PDE, the exponential potential and
the NormalHedge potential. We give the form of the potential function
that satisfies Kolmogorov Equation~\ref{eqn:Kolmogorov}, and derive
the regret bound corresponding to it.

{\bf The exponential potential function} which corresponds to exponential
  weights algorithm corresponds to the following equation
\[
    \pot_{\mbox{exp}}(\R,t) = e^{\sqrt{2} \eta \R - \eta^2 t}
  \]
  Where $\eta>0$ is the learning rate parameter.
  
Given $\epsilon$ we choose $\eta = \sqrt{\frac{\ln (1/\epsilon)}{t}}$
we get the regret bound that holds for any $t>0$
  \begin{equation}
    \R_\epsilon \leq \sqrt{2 t \ln \frac{1}{\epsilon}}
  \end{equation}
Note that the algorithm depends on the choice of $\epsilon$, in other
words, the bound does {\em not} hold for all values of $\epsilon$ at
the same time.

{\bf The NormalHedge value} is
\begin{equation} \label{eqn:NormalHedge}
  \pot_{\mbox{NH}}(\R,t) = \begin{cases}
    \frac{1}{\sqrt{t+\nu}}\exp\left(\frac{\R^2}{2(t+\nu)}\right)
    & \mbox{if } \R \geq 0  \\
  \frac{1}{\sqrt{t+\nu}} & \mbox{if } \R <0
  \end{cases}
\end{equation}
Where $\nu>0$ is a small constant. The function $\pot_{\mbox{NH}}(\R,t)$,
restricted to $\R\geq 0$ is in $\SP{4}$ and is a constant for $\R \leq 0$.

The regret bound we get is:
\begin{equation}
\R_\epsilon \leq \sqrt{(t+\nu) \left( \ln (t+\nu) + 2 \ln \frac{1}{\epsilon}\right)}
\end{equation}
This bound is slightly larger than the bound for exponential weights,
however, the NormalHedge bound holds simultanuously for all
$\epsilon>0$ and the algorithm requires no tuning.

\section{The continuous time game and bounds for easy sequences} \label{sec:easy}
In Section~\ref{sec:discrete} we have shown that the integer time game
has a natural extension to a setting where $\deltat_i = s_i^2$. We
also demonstrated sequences of adversarial strategies
$S_1,S_2,\ldots$ such that $\sup_{k \to \infty} {\lowerpot}_k(0,\R) = $

We characterized the optimal adversarial strategy for the discrete
time game (Section~\ref{sec:discrete-Time-Game}), which corresponds
to the adversary choosing the loss to be $s_i$ or $-s_i$ with equal
probabilities. A natural question at this point is to characterize the
regret when the adversary is not optimal, or the sequences are ``easy''.

To see that such an improvement is possible, consider the following
{\em constant} adversary. This adversary associates the same loss to
all experts on iteration $i$, formally, $\adversM(i,\R) = l$. In this
case the average loss is also equal to $l$, $\ell(i)=l$ which means
that all of the instantaneous regrets are $r=l-\ell(t_i) = 0$, which,
in turn, implies that $\state(i) = \state(i+1)$. As the state did not
change, it makes sense to set $t_{i+1}=t_i$, rather than
$t_{i+1}=t_i+s_i^2$.

We observe two extremes for the adversarial behaviour. The constant
adversary described above for which $t_{i+1} = t_i$, and the random walk adversary described
earlier, in which each expert is split into two, one half with loss
$-s_i$ and the other with loss $+s_i$. In which case $t_{i+1} =
t_i+s_i^2$ which is the maximal increase in $t$ that the adversary can
guarantee. The analysis below shows that these are two extremes on a
spectrum and that intermediate cases can be characterized using a
variance-like quantity.

We define a variant of the discrete time game
(\ref{sec:discrete-Time-Game}) For concreteness we include the
learner's strategy, which is the limit of the strategy in the discrete
game when $s_i \to 0$.


\begin{figure}[ht!]
\framebox{
\begin{minipage}[t]{6.4in}
Set $t_1=0$ \\
Fix maximal step $0<s<1$ \\
On iteration $i=1,2,\ldots$

\begin{enumerate}
\item  If $t_i=T$ the game terminates.
\item Given $t_{i}$, the learner chooses a distribution
  $\learnerM(i)$ over $\reals$:
  \begin{equation} \label{eqn:learner-strat-cc}
  \learnerM^{cc}(t,\R) =  \frac{1}{Z^{cc}}
  \left. \frac{\partial}{\partial r} \right|_{r=\R} \pot(t,r)
  \mbox{ where } Z^{cc} = \E{\R \sim \state(t_i)}{\left. \frac{\partial}{\partial r} \right|_{r=\R} \pot(t,r)}
\end{equation}

\item The adversay chooses a {\em step size} $0<s_i\leq s$ and a mapping from $\reals$ to distributions
  over $[-s_i,+s_i]$: $\adversM(t): \reals \to \Delta^{[-s_i,+s_i]}$
\item The aggregate loss is calculated:
  \begin{equation} \label{eqn:ell-discrete}
    \ell(t_i)=\E{\R \sim \state(t_i)}{\learnerM^{cc}(t_i,\R)
      \Bias(t_i,\R)},\;\mbox{ where } \Bias(t_i,\R) \doteq \E{y \sim \adversM(t_i,\R)}{y}
  \end{equation}
  the aggregate loss is restricted to $|\ell(t_i)| \leq c s_i^2$.
\item  Increment $t_{i+1} = t_{i} + \deltat_i$ where
\begin{equation} \label{eqn:deltat}
  \deltat_i=
  \E{\R \sim \state(t_i)}{H(t_i,\R) \;\; \E{y \sim \adversM(t_i,\R)}{(y-\ell(t_i))^2}}
\end{equation}
Where
\begin{equation}
 H(t_i,R)=\frac{1}{Z^H} \left. \frac{\partial^2}{\partial r^2} \right|_{r=\R} \pot(t_i,r)
  \mbox{ and } Z^H = \E{\R \sim \state(t_i)}{\left. \frac{\partial^2}{\partial r^2} \right|_{r=\R} \pot(t_i,r)}
\end{equation}

\item The state is updated.
  $$\state(t_{i+1}) = \E{\R \sim \state(t_{i})}{\adversM(t_i)(\R)\oplus (\R-\ell(t_i))}
  $$
\end{enumerate}
\end{minipage}}
\caption{The continuous time game and learner strategy\label{sec:contin-Time-Game}}
\end{figure}

Our characterization applies to the limit where the $s_i$ are small. Formally, we define
\begin{definition}
We say that an instance of the discrete time game is
$(n,s,\tau)$-bounded if it consists of $n$ iterations and $\forall\;\; 0<i\leq n,\;\; s_i < s$ and $\sum_{j=1}^n s_j^2=\tau$
\end{definition}

Note that $\tau>t_n$ and that $\tau$ depends only on the ranges $s_i$
while $t_n$ depends on the variance. $t_n = T$ 
is the dominant term in the regret bound, while $\tau$ controls the
error term.

\newpage

\begin{theorem} \label{thm:variancebound} Let $\pot \in \SP{\infty}$
  be a potential function that satisfies the Kolmogorov backward
  equation~(\ref{eqn:Kolmogorov}).
  Fix the total time $\tau$ and let $G_n$ be an $(n,
  \sqrt{\frac{\tau}{n}},\tau)$-bounded game. Let $n \to \infty$.

Then 
$$\score(\state(\tau)) \leq \score(\state(0))+O\paren{\frac{1}{\sqrt{n}}}$$
\end{theorem}

The proof is given in appendix~\ref{appendix:ProofOfVarianceBound}

If we define

\begin{equation} \label{eqn:Vn}
  V_n = t_n = \sum_{i=1}^n \deltat_i= 
  \sum_{i=1}^n \E{\R \sim \state(t_i)}{\E{y \sim \adversM(t_i,\R)}{H(t_i,\R) ((y-\ell(t_i))^2)}}
\end{equation}

We can use $V_n$ instead of $T$ giving us a variancee based bound.

  
%    \E{\R \sim \state(i)}{ \E{y \sim
%      \adversM(i)(\R)}{\pot(i,\R+y-\ell(i))}}


\bibliographystyle{plain}
\bibliography{ref.bib,bib.bib}

\appendix
\section{Proof of Theorem~\ref{thm:simulBoundAveragePot} \label{proof:simulBoundAveragePot}}
\proof
  \begin{itemize}
  \item
  {\bf $\state$ satisfies a simultanous bound for $B$ if it satisfies an
  average potential bound for $\pot = B^{-1}$}\\
Assume by contradiction that $\state$ does not satisfy the simultanous bound. In
other words there exists $a \in \reals$ such that
$\P{\R \sim \state}{\R > a} > B(a)$. From Markov inequality and the fact
that $\phi$ is non decreasing we get
\[
  \E{\R \sim \state}{\pot(\R)} \geq \phi(a) \P{\R \sim \state}{\R > a} >
  \phi(a) B(a) = \frac{B(a)}{B(a)}=1
\]
but $ \E{\R \sim \state}{\pot(\R)} >1$ contradicts the average potential
assumption for the potential $\phi(\R) = B(\R)^{-1}$
\item
{\bf $\state$ satisfies an
  average potential bound for $\pot = B^{-1}$ if it satisfies a simultanous bound for $B$}\\
As $\phi$ is a non-decreasing function, and assuming $\R,\R'$ are drawn
independently at random according to $\state$:
\begin{eqnarray}
  \E{\R \sim \state}{\pot(\R)} & = & \E{\R \sim \state}{\pot(\R)
                                  \P{\R' \sim \state}{\phi(R') \geq \phi(\R)}} \\
                            & \leq & \E{\R \sim \state}{\pot(\R)
                                     \P{\R' \sim \state}{R' \geq \R}} \\
                            & < & \E{\R \sim \state}{\pot(\R) B(\R)} \\
                            & = & \E{\R \sim \state}{\frac{B(\R)}{B(\R)}}
                                  = \E{\R \sim \state}{1} = 1
\end{eqnarray}
\end{itemize}
\qed

\section{Divided differences of a function} \label{sec:divdiff}


the lhs has the form
A function $\finalPot{}$ that satisfies
inequality~\ref{eqn:4thOrderConvex} is said to be {\em 4'th order convex}
(see details in in~\cite{butt2016generalization}).


Following\cite{butt2016generalization} we give a brief review of
divided differences and of $n$-convexity.

Let $f:[a,b] \to \reals$ be a function from the segment $[a,b]$ to the
reals.

\begin{definition}[$n$'th order divided difference of a function]
  The $n$'th order divided different of a function $f:[a,b] \to
  \reals$ at mutually distinct and ordered points $a \leq x_0 < x_1
  < \cdots < x_n \leq b$
  defined recursively by
  \[ [x_i; f] = f(x_i), \; i \in 0,\ldots n,\]
  \[ [x_0,\ldots,x_n;f] =
    \frac{[x_1,\ldots,x_n;f]-[x_0,\ldots,x_{n-1};f]}{x_n-x_0} \]
\end{definition}

\begin{definition}[$n$-convexity]
 A function $f:[a,b] \to \reals$ is said to be $n$-convex  $n \geq 0$
 if and only if for all choices of $n+1$ distinct points: $a \leq x_0 < x_1
  < \cdots < x_n \leq b$, $[x_0,\ldots,x_n;f]\geq 0$ holds.
\end{definition}
$n$-convexity is has a close connection to the sign of $f^{(n)}$ - the $n$'th
derivative of $f$, this connection was proved in 1965 by
popoviciu~\cite{popoviciu1965certaines}.
\begin{theorem} \label{thm:popo}
If $f^{(n)}$ exists then f is $n$-convex if and only if $f^{(n)}\geq 0$.
\end{theorem}

The next lemma states that the function $g(\R)>0$ as defined in
Equation~(\ref{eqn:divdiff}).

\proof {\bf of Lemma~(\ref{lemma:divdiff})

Fix $t$ and define $f(x) = \pot(t,x)$.
Let $(x_0,x_1,x_2,x_3,x_4)=(\R-2 s,\R-s,\R,\R+s,\R+2s)$

Using this notation we can rewrite $g(\R)$ in the form
\begin{equation}
  h(x_0,x_1,x_2,x_3,x_4) =  \frac{1}{24s^4} \paren{f(x_4)- 4f(x_3)+ 6f(x_2)-
    4f(x_1)+ f(x_0)}
\end{equation}
  
k

Is the 4-th order divided difference of $\pot(t,\cdot)$


\begin{enumerate}
\item
$$[x_i;f] = f(x_i)$$
\item
  $$[x_i,x_{i+1};f]=\frac{f(x_{i+1})-f(x_i)}{s}$$
\item
  $$[x_i,x_{i+1},x_{i+2};f] =
  \frac{\frac{f(x_{i+2})-f(x_{i+1})}{s}-\frac{f(x_{i+1})-f(x_i)}{s}}{2s}
  =\frac{f(x_{i+2})-2f(x_{i+1})+f(x_i)}{2 s^2}
  $$
\item
  \begin{eqnarray*}
    [x_i,x_{i+1},x_{i+2},x_{i+3};f]& = &
    \frac{\frac{f(x_{i+3})-2f(x_{i+2})+f(x_{i+1})}{2
    s^2}-\frac{f(x_{i+2})-2f(x_{i+1})+f(x_i)}{2
    s^2}}{3s}\\
    &=& \frac{f(x_{i+3}) -3f(x_{i+2})+3f(x_{i+1})-f(x_i)   }{6 s^3}
  \end{eqnarray*}
\item
  \begin{eqnarray*}
    [x_i,x_{i+1},x_{i+2},x_{i+3},x_{i+4};f]& = &
    \frac{\frac{f(x_{i+4}) -3f(x_{i+3})+3f(x_{i+2})-f(x_{i+1})   }{6 s^3}
    - \frac{f(x_{i+3}) -3f(x_{i+2})+3f(x_{i+1})-f(x_i)   }{6 s^3}}
    {4s}\\
    &=& \frac{f(x_{i+4})-4f(x_{i+3})+6f(x_{i+2})-4f(x_{i+1})+f(x_i)}{24s^4}
  \end{eqnarray*}
\end{enumerate}

\qed
\section{Proof of Theorem~\ref{thm:variancebound}}
\label{appendix:ProofOfVarianceBound}
We start with two technical lemmas
\begin{lemma} \label{lemma:infiniteexpectations}
Let $f(x) \in \SP{2}$, i.e. $f(x), f'(x),f''(x) >0$ for all $x \in
\reals$, let $h(x)$ be a uniformly bounded function: $\forall x,\;\; |h(x)|<1$.
Let $\state$ be a distribution over $\reals$.
If $\E{x \sim \state}{f(x)}$ is well-defined (and finite) , then 
$\E{x \sim \state}{h(x) f'(x)}$ is well defined (and finite) as well.
\end{lemma}
\proof
Assume by contradiction that $\E{x \sim \state}{h(x) f'(x)}$ is
undefined. Define $h^+(x) = \max(0,h(x))$.
As $f'(x)>0$, this implies that either $\E{x \sim \state}{h^+(x)
  f'(x)}=\infty$ or $\E{x \sim \state}{(-h)^+(x) f'(x)}=\infty$ (or both). 

Assue wlog that $\E{x \sim \state}{h^+(x) f'(x)}=\infty$. As
$f'(x)>0$ and $0 \leq h^+(x) \leq 1$ we get that $\E{x \sim
  \state}{f'(x)}=\infty$.
As $f(x+1) \geq f'(x)$ we get that $\E{x \sim
  \state}{f(x)}=\infty$ which is a contradiction.
\qed


\newcommand{\Dx}{\Delta x}
\newcommand{\Dy}{\Delta y}
\begin{lemma} \label{lemma:Taylor2D}
Let $f(x,y)$ be a differentiable function with continuous derivatives
up to degree three. Then
\begin{eqnarray}
  &&f(x_0+\Dx,y_0+\Dy) = f(x_0,y_0)
  + \atI{\frac{\partial}{\partial x}} \Dx 
  + \atI{\frac{\partial}{\partial y}} \Dy \\
  &+&\frac{1}{2} \atI{\frac{\partial^2}{\partial x^2}} \Dx^2
      +\atI{\frac{\partial^2}{\partial x\partial y}} \Dx\Dy
      +\frac{1}{2} \atI{\frac{\partial^2}{\partial y^2}} \Dy^2\\
  &+&\frac{1}{6} \atII{\frac{\partial^3}{\partial x^3}} \Dx^3
      +\frac{1}{2} \atII{\frac{\partial^3}{\partial x^2 \partial y}} \Dx^2\Dy\\
  &&+ \frac{1}{2} \atII{\frac{\partial^3}{\partial x \partial y^2}} \Dx\Dy^2
    + \frac{1}{6} \atII{\frac{\partial^3}{\partial y^3}} \Dy^3
\end{eqnarray}
for some $0\leq t \leq 1$.
\end{lemma}
\proof {\em of Lemma~\ref{lemma:Taylor2D}} 
Let $F:[0,1] \to \reals$ be defined as  $F(t)=f(x(t),y(t))$ where
$x(t) = x_0+t\Dx$ and $y(t)=y_0+t\Dy$. Then $F(0)=f(x_0,y_0)$ and
$F(1)=f(x_0+\Dx,y_0+\Dy)$. It is easy to verify that
$$ \frac{d}{dt}F(t)
=\frac{\partial}{\partial x} f(x(t),y(t))\Dx
+ \frac{\partial}{\partial y} f(x(t),y(t))\Dy
$$
and that in general:
\begin{equation} \label{eqn:d.dn.F}
\frac{d^n}{d t^n} F(t) = \sum_{m=1}^n {n \choose m}
\frac{\partial^n}{\partial x^m \partial y^{n-m}} f(x_0+t \Dx,y_0+t\Dy)
\Dx^m \Dy^{n-m}
\end{equation}
As $f$ has partial derivatives up to degree 3, so does $F$. Using the
Taylor expansion of $F$ and the intermediate point theorem we get that
\begin{equation} \label{eqn:Taylor.F}
  f(x_0+\Dx,y_0+\Dy) = F(1) = F(0)+\frac{d}{dt}F(0)
  +\frac{1}{2}\frac{d^2}{dt^2}F(0)
  +\frac{1}{6}\frac{d^3}{dt^3}F(t')
\end{equation}
Where $0 \leq t' \leq 1$. Using Eqn~(\ref{eqn:d.dn.F}) to expand each
term in Eqn.~(\ref{eqn:Taylor.F}) completes the proof.
\qed


\proof {\em of Theorem~\ref{thm:variancebound}}\\
We prove the claim by an upper bound on the increase of potential that holds for any iteration $1 \leq i \leq n$:
\begin{equation} \label{proof:onestep}
\score(\state(t_{i+1})) \leq \score(\state(t_i)) + a s_i^3 \mbox{ for some constant } a>0
\end{equation}
Summing inequality~(\ref{proof:onestep}) over all iterations we get that 
\begin{equation} \label{proof:allsteps}
\score(\state(T)) \leq \score(\state(0)) + c \sum_{i=1}^n s_i^3 \leq 
\score(\state(0)) + a s \sum_{i=1}^n s_i^2 = 
\score(\state(0)) + a s T
\end{equation}
From which the statement of the theorem follows.

We now prove inequality~(\ref{proof:onestep}). 
We use the notation $r=y -\ell(i)$ to denote the instantaneous regret at iteration $i$. 


Applying Lemma~\ref{lemma:Taylor2D} to
$\pot(t_{i+1},\R_{i+1})=\pot(t_i+\deltat_i,\R_i+r_i)$  we get
\begin{eqnarray} 
    \pot(t_i+\deltat_i,\R_i+r_i) & =&  
    \pot(t_i,\R_i)\\
    &+&\at{\frac{\partial}{\partial \rho}} r_i \\
    &+&\at{\frac{\partial}{\partial \tau}}  \deltat_i \\
    &+& \frac{1}{2} \at{\frac{\partial^2}{\partial \rho^2}} r_i^2 \\
    &+& \at{\frac{\partial^2}{\partial r \partial \tau}} r_i \deltat_i \label{term:Taylor_rdt}\\
    &+& \frac{1}{2} \at{\frac{\partial^2}{\partial \tau^2}} \deltat_i^2 \label{term:Taylor_dtsquare}\\
    &+& \frac{1}{6} \att{\frac{\partial^3}{\partial \rho^3}} r_i^3 \label{term:Taylor_r3}\\
    &+& \frac{1}{2} \att{\frac{\partial^3}{\partial \rho^2 \partial \tau}} r_i^2\deltat_i \label{term:Taylor_r2t}\\
    &+& \frac{1}{2} \att{\frac{\partial^3}{\partial \rho \partial \tau^2}} r_i\deltat_i^2 \label{term:Taylor_rt2}\\
    &+& \frac{1}{6} \att{\frac{\partial^3}{\partial \tau^3}} \deltat_i^3 \label{term:Taylor_t3}
\end{eqnarray}
for some $0 \leq g \leq 1$.

By assumption $\pot$ satisfies the Kolmogorov backward equation:
\begin{equation*} 
  \frac{\partial}{\partial \tau} \pot(\tau,\rho)
  = -\frac{1}{2} \frac{\partial^2}{\partial r^2} \pot(\tau,\rho)
\end{equation*}
Combining this equation with the exchangability of the order of
partial derivative (Clairiaut's Theorem) we can substitute all
partial derivatives with respect to $\tau$ with partial derivatives
with respect to $\rho$ using the following equation.
\[
  \frac{\partial^{n+m}}{\partial \rho^n \partial \tau^m} \pot(\tau,\rho)=
  (-1)^m \frac{\partial^{n+2m}}{\partial \rho^{n+2m}} \pot(\tau,\rho)
\]
Which yields
\begin{eqnarray}
      \pot(t_i+\deltat_i,\R_i+r_i) & =&  
    \pot(t_i,\R_i)\\
    &+& \at{\frac{\partial}{\partial \rho}} r_i \label{term:coll1}\\
    &+& \at{\frac{\partial^2}{\partial \rho^2}} \paren{\frac{r_i^2}{2}-\deltat_i} \label{term:coll2}\\
    &-& \at{\frac{\partial^3}{\partial \rho^3}} r_i \deltat_i \label{term:coll3}\\
    &+& \frac{1}{2} \at{\frac{\partial^4}{\partial \rho^4}} \deltat_i^2 \label{term:coll4}\\
    &+& \frac{1}{6} \att{\frac{\partial^3}{\partial \rho^3}} r_i^3 \label{term:coll5}\\
    &-& \frac{1}{2} \att{\frac{\partial^4}{\partial \rho^4}} r_i^2\deltat_i \label{term:coll6}\\
    &+& \frac{1}{2} \att{\frac{\partial^5}{\partial \rho^5}} r_i\deltat_i^2 \label{term:coll7}\\
    &-& \frac{1}{6} \att{\frac{\partial^6}{\partial \rho^6}} \deltat_i^3 \label{term:coll8}
\end{eqnarray}

  From the assumption that the game is $(n,s,T)$-bounded we get that 
  \begin{enumerate}
  \item $|r_i| \leq s_i +c s_i^2 \leq 2 s_i$
  \item $\deltat_i \leq s_i^2 \leq s^2$
    % \item $\sum_i \deltat_i=T$
  \end{enumerate}

  given these inequalities we can rewrite the second factor in each
  term as follows, where $|h_i(\cdot)|\leq 1$
  \begin{itemize}
  \item {\bf For~(\ref{term:coll1}):}
    $r_i=2s_i\frac{r_i}{2s_i}=2s_ih_1(r_i)$.
  \item {\bf For~(\ref{term:coll2}):}
    $r_i^2 - \frac{1}{2}\deltat_i = 4s_i^2\frac{r_i^2 -
      \frac{1}{2}\deltat_i}{4s_i^2} = 4s_i^2 h_2(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll3}):} $r_i \deltat_i = 2s_i^3
    \frac{r_i \deltat_i}{2s_i^3} = 2s_i^3 h_3(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll4}):} $\deltat_i^2 =
    s_i^4\frac{\deltat_i^2}{s_i^4} = s_i^3 h_4(\deltat_i)$
  \item {\bf For~(\ref{term:coll5}):} $r_i^3 = 8s_i^3
    \frac{r_i^3}{8s_i^3} = 8s_i^3 h_5(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll6}):} $r_i^2 \deltat_i = 4s_i^4
    \frac{r_i^2 \deltat_i}{4s_i^4} = 4s_i^3 h_6(r_i,\deltat_i)$
  \item {\bf For~(\ref{term:coll7}):} $r_i \deltat_i^2 = 2s_i^5
    \frac{r_i \deltat_i^2}{2s_i^5}$
  \item {\bf For~(\ref{term:coll8}):} $\deltat_i^3 = s_i^6 \frac{\deltat_i^3}{s_i^6}$
\end{itemize}
  We therefor get the simplified equation
  
  \begin{eqnarray*} 
     \pot(t_i+\deltat_i,\R_i+r_i) & =&  \pot(t,\R)+\at{\frac{\partial}{\partial r}} r
    + \at{\frac{\partial}{\partial t}} \deltat \\
                                  &+& 
                                      \frac{1}{2}  \at{\frac{\partial^2}{\partial r^2}} r^2\\
                                  &+& \at{\frac{\partial^2}{\partial r \partial t}} r_i \deltat_i \label{term:Taylor_collected_rdt}\\
                                  &+& \frac{1}{6} \at{\frac{\partial^3}{\partial r^3}} r_i^3 \label{term:Taylor_collected_r3}
                                      + O(s^4)
\end{eqnarray*}

and therefor
  \begin{eqnarray} 
     \pot(t_i+\deltat_i,\R+r) &=& \pot(t_i,\R) +
                                  \at{\frac{\partial}{\partial r}} r
                                  \nonumber \\
    &+& \at{\frac{\partial^2}{\partial r^2}} (r^2 - \deltat_i) +
        O(s^3) \label{eqn:Taylor}
\end{eqnarray}

Our next step is to consider the expected value of~(\ref{eqn:Taylor}) wrt $\R \sim \state(t_i)$,
$y \sim \adversM(t_i,\R)$ for an arbitrary adversarial strategy
$\adversM$.

We will show that the expected potential does not increase:
\begin{equation} \label{eqn:deltatislargeenough}
     \E{\R \sim \state(t_i)}{ \E{y \sim \adversM(t_i,\R)}{\pot(t_i+\deltat_i,\R+y-\ell(t_i))}} \leq \E{\R \sim \state(t_i)}{\pot(t_i,\R)}
\end{equation}

Plugging Eqn~(\ref{eqn:Taylor}) into the LHS of
Eqn~(\ref{eqn:deltatislargeenough}) we get
\begin{eqnarray}
  \lefteqn{\E{\R \sim \state(t_i)}{\E{y \sim \adversM(t_i,\R)}{\pot(t_i+\deltat_i,\R+y-\ell(t_i))}}} \\
  &=& \E{\R \sim \state(t_i)}{\pot(t_i,\R)} \label{eqn:contin0}\\
  &+& \E{\R \sim \state(t_i)}{\E{y \sim \adversM(t_i,\R)}{\at{\frac{\partial}{\partial r}} (y-\ell(t_i))}} \label{eqn:contin1}\\
  &+& \E{\R \sim \state(t_i)}{\E{y \sim
      \adversM(t_i,\R)}{\at{\frac{\partial^2}{\partial r^2}}
      ((y-\ell(t_i))^2 - \deltat_i)}}
  \label{eqn:contin2}\\
  &+& O(s^3) \label{eqn:contin3}
\end{eqnarray}
Some care is needed here. we need to show that the expected value
are all finite. We assume that the expected potential
(Eqn~({eqn:contin0}) is finite. Using
Lemma~\ref{lemma:infiniteexpectations} this implies that the expected
value of higher derivatives of $\frac{\partial}{\partial \R} \pot(\R)$
are also finite.\footnote{I need to clean this up and find an argument
  that the expected value for mixed derivatives is also finite.}


To prove inequality~(\ref{proof:onestep}), we need to show that the
terms~\ref{eqn:contin1} and \ref{eqn:contin2} are smaller or equal to
zero.
~\\~\\~\\
{\bf Term~(\ref{eqn:contin1}) is equal to zero:}\\
As $\ell(t_i)$ is a constant
relative to $\R$ and $y$, and $\at{\frac{\partial}{\partial r}}$ is a
constant with respect to $y$ we can rewrite~(\ref{eqn:contin1}) as
\begin{equation} \label{eqnterm1.1}
  \E{\R \sim \state(t_i)}{\at{\frac{\partial}{\partial r}}
    \E{y \sim \adversM(t_i,\R)}{y} }
- \ell(t_i) \E{\R \sim \state(t_i)}{\at{\frac{\partial}{\partial r}}}
\end{equation}

Combining the definitions of $\ell(t)$~(\ref{eqn:ell-discrete}) and~
and the learner's strategy
$\learnerM^{cc}$~(\ref{eqn:learner-strat-cc}) we get that
\begin{eqnarray}
\ell(t_i) &=& \E{\R \sim \state{t_i}}{\frac{1}{Z}
              \at{\frac{\partial}{\partial r}}
              \E{y \sim \adversM(i,\R)}{y}} \mbox{ where }
              Z=\E{\R \sim \state{t_i}}{\frac{1}{Z}
              \at{\frac{\partial}{\partial r}}}
              \label{eqnterm1.2}
\end{eqnarray}

Plugging~(\ref{eqnterm1.2}) into (\ref{eqnterm1.1}) and recalling the
requirement that $\ell(t_i)<\infty$ we find that
term~(\ref{eqn:contin1}) is equal to zero.


~\\~\\~\\
{\bf Term~(\ref{eqn:contin2}) is equal to zero:}\\
As $\deltat_i$ is a constant relative to $y$, we can take it
outside the expectation and plug in the definition of $\deltat_i$ (\ref{eqn:deltat})
\begin{equation} \label{eqn:term2.1}
  \E{\R \sim \state(t_i)}{\E{y \sim
      \adversM(t_i,\R)}{\adversM(t_i,\R)}{\at{\frac{\partial^2}{\partial r^2}}
      (y-\ell(t_i))^2} - \deltat_i}=
  \deltat_i - \deltat_i =0
\end{equation}
Where $G(t_i,\R)$ is defined in Equation~(\ref{eqn:SecondOrderDerivative})
We find that (\ref{eqn:contin2}) is zero.

Finally (\ref{eqn:contin3}) is negligible relative to the other terms
as $s \to 0$.
\qed 

\end{document}

%% deleted stuff


The next Lemma is the main part of the proof ot
Theorem~(\ref{thm:IntegerGameBounds}). We use the backward induction
from Theorem~(\ref{thm:backward-recursion}) To compute upper and lower
potentials (Equations~(\ref{eqn:upperPotentials},\ref{eqn:lowerPotentials})) for
Strategies~(\ref{eqn:adv-strat-p}) and~(\ref{eqn:learner-strat-1})

The last iteration of the game: $i=T$ is the first step of the
backward induction. The uper and lower bounds are both set equal to
the first step in the backward induction we define
$$  \lowerpotb(T,\R) = \upperpotb(T,\R) = \pot(T,\R) $$

\iffalse
\begin{lemma} \label{lemma:first-order-bound}
  If $\pot(i,\R) \in \SP{2}$
  \begin{enumerate}
    \item The adversarial strategy~(Eqn~(\ref{eqn:adv-strat-p}))
    guarantees the lower potential
 \begin{equation} \label{eqn:backward-iteration-lower}
   \lowerpotb(i, \R) = \frac{\lowerpotb(i,\R+1) + \lowerpotb(i,\R-1)}{2}
 \end{equation}
   
    \item The learner strategy~(Eqn~(\ref{eqn:learner-strat-1}))
      guarantees the upper potential 
      \begin{equation} \label{eqn:backward-iteration-upper-recursion}
        \upperpotb(i, \R) = \frac{\upperpotb(i,\R+2) + \upperpotb(i,\R-2)}{2}
      \end{equation}
    \end{enumerate}
\end{lemma}
\fi


  \begin{eqnarray}
  \upperpotd(t_i,\R)&=&\E{\R \sim \state(i)}{\E{y \sim \adversM(t_i,\R)}{\upperpotd(t_i,\R+y-\ell(t_i))}}\\
  &\leq & \E{\R \sim \state(t_i)}{\frac{\upperpotd(i,\R+s_k(1+s_k))+\upperpotd(i,\R-s_k(1+s_k))}{2}}\\
  &+&
      \E{\R \sim \state(i)}{\E{y \sim \adversM(i)(\R)}{(y-\ell(i))
      \frac{\upperpotd(i,\R+s_k+s_k^2)+\upperpotd(i,\R-s_k-s_k^2)}{2}}}
  \end{eqnarray}


(Eqn~\ref{eqn:learner-strat-1}). We follow the same line of argument as the second part of the proof of
Lemma~\ref{lemma:first-order-bound} to give a recursion for the upper
potential. The citical difference between the integer game is and the
dicrete game is that in the discrete game $\ell(t_i)\leq s_i^2$ which
implies that $(y-\ell(t_i)) \in [-s_k(1+s_k),s_k(1+s_k)]$. This yields 


  Following the same line of argument as the first part of the proof of
Lemma~\ref{lemma:first-order-bound} we consider the time points:
$t_i=i s_k^2=i 2^{-2k}\realT$ for $i=0,1,\ldots,2^{2k}$.

  \begin{equation} \label{eqn:lower-discrete}
    \lowerpotd(t_{i-1},\R) = \frac{\lowerpotd(t_i,\R-s_k)+\lowerpotd(t_i,\R+s_k)}{2}
  \end{equation}


  %%%%%

  
Follow the proof of
  Lemma~\ref{lemma:first-order-bound}.Derive upper and lower scores
  for the two strategies. Show that they converge to the same thing as
  $k \to \infty$.




We start with the high-level idea. Consider iteration $i$ of the
continuous time game. We know that the adversary prefers $s_i$ to be
as small as possible. On the other hand, the adversary has to choose
some $s_i>0$. This means that the adversary always plays
sub-optimally. Based on $s_i$ the learner makes a choice and the
adversary makes a choice. As a result the current state $\state(t_{i})$
is transformed to $\state(t_i)$. To choose it's strategy, the learner
needs to assign value possible states $\state(t_i)$. How can she do
that? By assuming that in the future the adversary will play
optimally, i.e. setting $s_i$ arbitrarily small. While the adversary
cannot be optimal, it can get arbitrarily close to optimal, which is
brownian motion.

Note that the learner chooses a distribution {\em after} the adversary
set the value of $s_i$. The discrete time version of $\learnerM^1$

In the discrete time game the adversary has an additional choice, the
choice of $s_i$. Thus the adversary's strategy includes that choice.
There are two constraints on this choice: $s_i \geq 0$ and
$\sum_{i=1}^n s_i^2 = T$. Note that even that by setting $s_i$
arbitrarily small, the adversary can make the number of steps - $n$ -
arbitrarily large. We will therefor not identify a single adversarial
strategy but instead consider the supremum over an infinite sequence
of strategies.
